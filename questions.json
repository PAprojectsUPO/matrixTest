{
  "quizModules": [
    {
      "id": "algebra_lineal_1",
      "name": "Álgebra Lineal I",
      "description": "Preguntas sobre matrices, determinantes, y sistemas de ecuaciones lineales.",
      "difficultyLevels": {
        "basico": [
          {
            "id": "AL_BAS_001",
            "subCategory": "Tipos de Matrices",
            "text": "¿Cuál es la característica principal de una matriz nula?",
            "hint": "Piensa en una matriz cuyos elementos no alteran otras matrices al sumarse.",
            "options": [
              "Todos sus elementos son uno.",
              "Todos sus elementos son cero.",
              "Tiene una sola fila.",
              "Tiene una sola columna."
            ],
            "correctAnswer": "Todos sus elementos son cero.",
            "explanation": "Una matriz nula (denotada usualmente como 0) tiene todos sus elementos iguales a cero. Es análoga al número 0 en las operaciones matriciales, ya que para cualquier matriz A del mismo tamaño, se cumple que A + 0 = A."
          },
          {
            "id": "AL_BAS_002",
            "subCategory": "Tipos de Matrices",
            "text": "Si una matriz $A$ tiene orden $m × n$ y $m = n$, ¿qué tipo de matriz es?",
            "hint": "Piensa en matrices donde el número de filas y columnas permite operaciones especiales como el cálculo de determinantes.",
            "options": [
              "Matriz Rectangular.",
              "Matriz Fila.",
              "Matriz Cuadrada.",
              "Matriz Columna."
            ],
            "correctAnswer": "Matriz Cuadrada.",
            "explanation": "Una matriz cuadrada (donde $m = n$) es fundamental en álgebra lineal porque permite operaciones exclusivas como el cálculo de determinantes, trazas y potencias matriciales. Ejemplos clásicos son la matriz identidad y las matrices simétricas."
          },
          {
            "id": "AL_BAS_003",
            "subCategory": "Notación y Conceptos",
            "text": "¿Cómo se denota el conjunto de todas las matrices reales de orden $m×n$?",
            "hint": "La notación incluye información sobre las dimensiones y el campo numérico al que pertenecen los elementos.",
            "options": ["$M_n (R)$", "$R^{m×n}$", "$M_{m×n} (R)$", "$R^{n×m}$"],
            "correctAnswer": "$M_{m×n} (R)$",
            "explanation": "La notación $M_{m×n} (R)$ especifica: $m×n$ (orden), $R$ (números reales). Esta convención es consistente con la notación de espacios vectoriales de matrices, donde $R$ indica el cuerpo base y los subíndices las dimensiones."
          },
          {
            "id": "AL_BAS_004",
            "subCategory": "Tipos de Matrices",
            "text": "¿Qué propiedad define a una matriz identidad de orden $n$?",
            "hint": "Es la matriz equivalente al número 1 en la multiplicación matricial. ¿Cómo afecta al multiplicarla por otra matriz?",
            "options": [
              "Todos sus elementos son cero.",
              "Es una matriz diagonal con elementos de la diagonal principal iguales a cero.",
              "Es una matriz diagonal con elementos de la diagonal principal iguales a uno.",
              "Todos sus elementos son iguales."
            ],
            "correctAnswer": "Es una matriz diagonal con elementos de la diagonal principal iguales a uno.",
            "explanation": "La matriz identidad $I_n$ es crucial porque actúa como elemento neutro en la multiplicación de matrices: $A·I_n = A$ para cualquier matriz $A$ de orden compatible. Su estructura diagonal con unos garantiza esta propiedad."
          },
          {
            "id": "AL_BAS_005",
            "subCategory": "Notación y Conceptos",
            "text": "¿Cuándo se dice que dos matrices $A$ y $B$ son iguales?",
            "hint": "No basta con que tengan la misma forma; deben coincidir en todos sus componentes.",
            "options": [
              "Cuando tienen el mismo número de filas.",
              "Cuando tienen el mismo número de columnas.",
              "Cuando tienen el mismo orden y sus elementos correspondientes son iguales.",
              "Cuando sus determinantes son iguales."
            ],
            "correctAnswer": "Cuando tienen el mismo orden y sus elementos correspondientes son iguales.",
            "explanation": "La igualdad matricial exige: 1) Misma dimensión ($m×n$), y 2) $a_{ij} = b_{ij}$ para todo $i,j$. Esto es análogo a la igualdad de vectores, extendido a dos dimensiones."
          },
          {
            "id": "AL_BAS_006",
            "subCategory": "Tipos de Matrices",
            "text": "Una matriz con elementos dispuestos en una única fila se denomina:",
            "hint": "Es el análogo matricial de un vector horizontal.",
            "options": [
              "Matriz Columna",
              "n-upla",
              "Matriz Fila",
              "Matriz Rectangular"
            ],
            "correctAnswer": "Matriz Fila",
            "explanation": "Una matriz fila (1×n) representa aplicaciones lineales de ℝⁿ a ℝ. Es esencial en sistemas de ecuaciones y programación lineal, donde cada fila puede codificar restricciones."
          },
          {
            "id": "AL_BAS_007",
            "subCategory": "Tipos de Matrices",
            "text": "¿Cuál de los siguientes no es un tipo de matriz cuadrada según la diagonal principal?",
            "hint": "El nombre de este tipo de matriz hace referencia explícita a la desigualdad entre sus dimensiones.",
            "options": [
              "Matriz Escalar",
              "Matriz Identidad",
              "Matriz Rectangular",
              "Matriz Diagonal"
            ],
            "correctAnswer": "Matriz Rectangular",
            "explanation": "Las matrices rectangulares (m≠n) no pueden ser cuadradas por definición. Las otras opciones son subclases de matrices cuadradas donde la diagonal principal juega un rol central en sus propiedades."
          },
          {
            "id": "AL_BAS_008",
            "subCategory": "Notación y Conceptos",
            "text": "Si a una matriz $A$ se le elimina $m−k$ filas y $n−r$  columnas, ¿qué se obtiene?",
            "hint": "Es análogo a seleccionar un subconjunto de filas y columnas para formar una nueva matriz.",
            "options": [
              "Una matriz transpuesta",
              "Una matriz inversa",
              "Una submatriz",
              "Una matriz reducida por filas"
            ],
            "correctAnswer": "Una submatriz",
            "explanation": "Las submatrices son fundamentales para calcular menores y rangos. Por ejemplo, el determinante de una matriz se define recursivamente usando submatrices."
          },
          {
            "id": "AL_BAS_009",
            "subCategory": "Operaciones Elementales",
            "text": "La operación elemental de tipo 2 en filas consiste en:",
            "hint": "Esta operación preserva la equivalencia de matrices y es reversible mediante otro escalar.",
            "options": [
              "Intercambiar dos filas.",
              "Sumar una fila a otra multiplicada por un escalar.",
              "Sustituir una fila por ella misma multiplicada por un escalar no nulo.",
              "Sustituir una fila por ella misma más un escalar."
            ],
            "correctAnswer": "Sustituir una fila por ella misma multiplicada por un escalar no nulo.",
            "explanation": "Multiplicar una fila por λ≠0 (operación tipo 2) es clave en la eliminación gaussiana. Su reversibilidad (usando 1/λ) garantiza que no se pierde información durante el proceso."
          },
          {
            "id": "AL_BAS_010",
            "subCategory": "Notación y Conceptos",
            "text": "¿Cómo se denota el rango de una matriz $A$?",
            "hint": "Esta notación proviene del término inglés 'rank' y es independiente del lenguaje de programación.",
            "options": ["$det(A)$", "$A^{-1}$", "$ran(A)$", "$A^{T}$"],
            "correctAnswer": "$ran(A)$",
            "explanation": "El rango ($ran(A)$) indica la dimensión del espacio generado por sus filas/columnas. Es crucial para determinar soluciones de sistemas lineales: si $ran(A)=ran([A|b])$, el sistema es compatible."
          },
          {
            "id": "AL_BAS_011",
            "subCategory": "Tipos de Matrices",
            "text": "¿Qué tipo de matriz tiene sus elementos dispuestos en una única columna?",
            "hint": "Es el análogo matricial de un vector vertical, común en sistemas de ecuaciones.",
            "options": [
              "Matriz Fila",
              "Matriz Columna",
              "Matriz Cuadrada",
              "Matriz Nula"
            ],
            "correctAnswer": "Matriz Columna",
            "explanation": "Las matrices columna (m×1) representan vectores en ℝᵐ. Son esenciales en ecuaciones lineales $Ax=b$, donde $b$ es una matriz columna."
          },
          {
            "id": "AL_BAS_012",
            "subCategory": "Tipos de Matrices",
            "text": "Si una matriz es diagonal y sus elementos de la diagonal principal son iguales, ¿cómo se denomina?",
            "hint": "Este tipo de matriz es un múltiplo escalar de la matriz identidad.",
            "options": [
              "Matriz Identidad",
              "Matriz Nula",
              "Matriz Escalar",
              "Matriz Triangular"
            ],
            "correctAnswer": "Matriz Escalar",
            "explanation": "Una matriz escalar ($\\lambda I$) conmuta con todas las matrices del mismo orden. En geometría, representan homotecias (transformaciones de escala uniforme)."
          },
          {
            "id": "AL_BAS_013",
            "subCategory": "Operaciones con Matrices",
            "text": "¿Qué condición deben cumplir dos matrices para poder sumarse?",
            "hint": "La suma requiere compatibilidad dimensional en ambas direcciones.",
            "options": [
              "Tener el mismo número de filas.",
              "Tener el mismo número de columnas.",
              "Tener el mismo orden.",
              "Ser matrices cuadradas."
            ],
            "correctAnswer": "Tener el mismo orden.",
            "explanation": "La suma matricial se define componente a componente, por lo que ambas matrices deben tener idénticas dimensiones ($m×n$). Esta restricción no aplica para el producto matricial."
          },
          {
            "id": "AL_BAS_014",
            "subCategory": "Operaciones con Matrices",
            "text": "¿Cómo se obtiene el producto de una matriz por un escalar $k$?",
            "hint": "Cada elemento de la matriz se ve afectado de la misma manera que en el producto de un vector por un escalar.",
            "options": [
              "Multiplicando solo la primera fila por $k$.",
              "Sumando $k$ a cada elemento.",
              "Multiplicando cada elemento de la matriz por $k$.",
              "Multiplicando solo la diagonal principal por $k$."
            ],
            "correctAnswer": "Multiplicando cada elemento de la matriz por $k$.",
            "explanation": "El producto por escalar ($kA$) es una homotecia lineal. Preserva la estructura de la matriz y es distributivo respecto a la suma matricial: $k(A+B) = kA + kB$."
          },
          {
            "id": "AL_BAS_015",
            "subCategory": "Operaciones con Matrices",
            "text": "¿Cuál es el elemento neutro para la suma de matrices de orden $m\\times n$?",
            "hint": "Es análogo al cero en la suma de números, pero adaptado a la estructura matricial.",
            "options": [
              "La matriz identidad de orden $n$.",
              "La matriz nula de orden $m\\times n$.",
              "Cualquier matriz diagonal.",
              "La matriz con todos los elementos iguales a uno."
            ],
            "correctAnswer": "La matriz nula de orden $m\\times n$.",
            "explanation": "La matriz nula $0_{m×n}$ es el único elemento que satisface $A + 0 = A$ para toda matriz $A$ de orden $m×n$. En espacios vectoriales de matrices, cumple el rol del vector cero."
          }
        ],
        "intermedio": [
            {
              "id": "AL_INT_001",
              "subCategory": "Propiedades de Matrices",
              "text": "Si $A$ es una matriz simétrica, ¿qué relación existe entre $A$ y su transpuesta $A^t$?",
              "hint": "Piensa en cómo se reflejan los elementos respecto a la diagonal principal.",
              "options": ["$A=-A^t$", "$A=A^t$", "$A\\cdot A^t=I_n$", "$A^2=A$"],
              "correctAnswer": "$A=A^t$",
              "explanation": "Una matriz simétrica es igual a su transpuesta ($A=A^t$), lo que implica que los elementos son simétricos respecto a la diagonal principal. Esto es fundamental en aplicaciones como formas cuadráticas y diagonalización."
            },
            {
              "id": "AL_INT_002",
              "subCategory": "Operaciones con Matrices",
              "text": "Dada una matriz $A∈M_{m×n} (R)$ y $B∈M_{n×r} (R)$, ¿cuál es el orden de la matriz producto $A⋅B$?",
              "hint": "El número de columnas de la primera matriz debe coincidir con el número de filas de la segunda.",
              "options": ["$m×n$", "$n×r$", "$m×r$", "$r×m$"],
              "correctAnswer": "$m×r$",
              "explanation": "El producto $A⋅B$ existe solo si el número de columnas de $A$ ($n$) coincide con el número de filas de $B$ ($n$). La matriz resultante tendrá las filas de $A$ y las columnas de $B$ ($m×r$)."
            },
            {
              "id": "AL_INT_003",
              "subCategory": "Operaciones con Matrices",
              "text": "¿Cuál de las siguientes afirmaciones es falsa con respecto al producto de matrices?",
              "hint": "Considera si el orden de multiplicación afecta el resultado en casos generales.",
              "options": [
                "El producto de matrices es asociativo.",
                "El producto de matrices es conmutativo en general.",
                "El producto de matrices es distributivo respecto a la suma.",
                "La matriz identidad es el elemento neutro para el producto."
              ],
              "correctAnswer": "El producto de matrices es conmutativo en general.",
              "explanation": "El producto matricial no es conmutativo ($A⋅B ≠ B⋅A$), excepto en casos especiales como matrices diagonales. Sin embargo, sí es asociativo y distributivo, con $I_n$ como elemento neutro."
            },
            {
              "id": "AL_INT_004",
              "subCategory": "Propiedades de Matrices",
              "text": "Si $A∈M_n (R)$ es una matriz ortogonal, ¿qué relación se cumple entre $A$ y su transpuesta $A^t$?",
              "hint": "Estas matrices preservan longitudes y ángulos en transformaciones lineales.",
              "options": ["$A=A^t$", "$A=-A^t$", "$A⋅A^t=I_n$", "$A^2=A$"],
              "correctAnswer": "$A⋅A^t=I_n$",
              "explanation": "Una matriz ortogonal cumple $A⋅A^t=I_n$, lo que implica que sus columnas/filas forman una base ortonormal. Son clave en rotaciones y reflexiones en álgebra lineal."
            },
            {
              "id": "AL_INT_005",
              "subCategory": "Propiedades de Matrices",
              "text": "¿Qué condición debe cumplir una matriz triangular para ser invertible?",
              "hint": "El determinante de una matriz triangular depende únicamente de ciertos elementos específicos.",
              "options": [
                "Todos sus elementos deben ser distintos de cero.",
                "Todos los elementos de su diagonal principal deben ser distintos de cero.",
                "Su determinante debe ser cero.",
                "Debe ser una matriz cuadrada."
              ],
              "correctAnswer": "Todos los elementos de su diagonal principal deben ser distintos de cero.",
              "explanation": "En una matriz triangular, el determinante es el producto de los elementos diagonales. Por tanto, es invertible si y solo si todos estos elementos son no nulos."
            },
            {
              "id": "AL_INT_006",
              "subCategory": "Propiedades de Matrices",
              "text": "Si A es una matriz antisimétrica, ¿qué condición deben cumplir los elementos de su diagonal principal?",
              "hint": "¿Qué pasa si comparas un elemento diagonal consigo mismo en una matriz antisimétrica?",
              "options": [
                "Deben ser todos uno.",
                "Deben ser todos cero.",
                "Pueden ser cualquier valor real.",
                "Deben ser iguales entre sí."
              ],
              "correctAnswer": "Deben ser todos cero.",
              "explanation": "En una matriz antisimétrica ($A=−A^t$), los elementos diagonales cumplen $a_{ii}=−a_{ii} ⇒ a_{ii}=0$. Esto surge de la definición y es crucial en álgebra de Lie."
            },
            {
              "id": "AL_INT_007",
              "subCategory": "Operaciones con Matrices",
              "text": "¿Cuál de las siguientes propiedades es correcta para la traspuesta de la suma de dos matrices A y B?",
              "hint": "La transpuesta respeta operaciones lineales básicas de manera específica.",
              "options": [
                "$(A+B)^t=A^t−B^t$",
                "$(A+B)^t=B^t⋅A^t$",
                "$(A+B)^t=A^t+B^t$",
                "$(A+B)^t=A⋅^t$"
              ],
              "correctAnswer": "$(A+B)^t=A^t+B^t$",
              "explanation": "La transpuesta de una suma es la suma de las transpuestas: $(A+B)^t=A^t+B^t$. Esta propiedad lineal es fundamental para demostraciones en espacios vectoriales de matrices."
            },
            {
              "id": "AL_INT_008",
              "subCategory": "Tipos de Matrices",
              "text": "Si una matriz $A∈M_n (R)$ es tal que $a_{ij} =0$ para todo $i>j$, ¿qué tipo de matriz es?",
              "hint": "Visualiza una matriz donde todos los elementos por debajo de la diagonal principal son nulos.",
              "options": [
                "Matriz Triangular Inferior",
                "Matriz Diagonal",
                "Matriz Triangular Superior",
                "Matriz Escalar"
              ],
              "correctAnswer": "Matriz Triangular Superior",
              "explanation": "Una matriz triangular superior tiene ceros debajo de la diagonal principal ($a_{ij}=0$ para $i>j$). Estas matrices simplifican cálculos de determinantes y sistemas lineales."
            },
            {
              "id": "AL_INT_009",
              "subCategory": "Determinantes",
              "text": "¿Cuál es la propiedad del determinante de una matriz $A$ si se intercambian dos de sus filas?",
              "hint": "Considera cómo afecta la permutación de filas a la orientación del volumen asociado.",
              "options": [
                "El determinante no cambia.",
                "El determinante se multiplica por $k$.",
                "El determinante cambia de signo.",
                "El determinante se hace cero."
              ],
              "correctAnswer": "El determinante cambia de signo.",
              "explanation": "Intercambiar filas cambia el signo del determinante, ya que altera la orientación del paralelepípedo definido por los vectores fila. Esto refleja la propiedad antisimétrica del determinante."
            },
            {
              "id": "AL_INT_010",
              "subCategory": "Determinantes",
              "text": "Si $A∈M_n (R)$ es una matriz invertible, ¿a qué es igual el determinante de su inversa, $∣A^{-1}∣$?",
              "hint": "Recuerda la relación entre el determinante de un producto y el producto de determinantes.",
              "options": ["$∣A∣$", "−$∣A∣$", "1/$∣A∣$", "$∣A∣^n$"],
              "correctAnswer": "1/$∣A∣$",
              "explanation": "Como $A⋅A^{-1}=I_n$ y $∣I_n∣=1$, se tiene que $∣A∣⋅∣A^{-1}∣=1 ⇒ ∣A^{-1}∣=1/∣A∣$. Esto muestra que solo matrices con determinante no nulo son invertibles."
            },
            {
              "id": "AL_INT_011",
              "subCategory": "Determinantes",
              "text": "¿Qué sucede con el determinante de una matriz si tiene una fila de ceros?",
              "hint": "Piensa en la expansión por cofactores a lo largo de una fila nula.",
              "options": [
                "El determinante es uno.",
                "El determinante cambia de signo.",
                "El determinante es cero.",
                "El determinante se mantiene igual."
              ],
              "correctAnswer": "El determinante es cero.",
              "explanation": "Una fila de ceros implica que el determinante es cero, ya que la expansión por cofactores a lo largo de esa fila da una suma de términos nulos. Esto también indica que la matriz no es invertible."
            },
            {
              "id": "AL_INT_012",
              "subCategory": "Determinantes",
              "text": "Si se multiplica una fila de una matriz A por un escalar $k$, ¿cómo cambia su determinante?",
              "hint": "El determinante es una función multilineal respecto a las filas.",
              "options": [
                "Se divide por $k$.",
                "Se multiplica por $k$ elevado al orden de la matriz.",
                "Se mantiene igual.",
                "Se multiplica por $k$."
              ],
              "correctAnswer": "Se multiplica por $k$.",
              "explanation": "Multiplicar una fila por $k$ escala el determinante por $k$, ya que el determinante es lineal en cada fila. Esto no aplica a operaciones entre filas."
            },
            {
              "id": "AL_INT_013",
              "subCategory": "Inversa de Matrices",
              "text": "¿Qué condición debe cumplir una matriz cuadrada $A$ para ser invertible a derecha y a izquierda simultáneamente?",
              "hint": "La invertibilidad está ligada a la existencia de soluciones únicas en sistemas lineales.",
              "options": [
                "Su determinante debe ser cero.",
                "Su determinante debe ser distinto de cero.",
                "Debe ser una matriz identidad.",
                "Debe ser una matriz nula."
              ],
              "correctAnswer": "Su determinante debe ser distinto de cero.",
              "explanation": "Una matriz es invertible (con inversa bilateral) si y solo si su determinante es no nulo. Esto garantiza rango completo y solución única para sistemas asociados."
            },
            {
              "id": "AL_INT_014",
              "subCategory": "Inversa de Matrices",
              "text": "¿Cuál es la relación entre el rango de una matriz $A$ de orden $m\\times n$ y su invertibilidad a derecha?",
              "hint": "La invertibilidad a derecha implica que el sistema $Ax=b$ tiene solución para cualquier $b$.",
              "options": [
                "$ran(A) = n$",
                "$ran(A) = m$",
                "$ran(A) < m$",
                "$ran(A) < n$"
              ],
              "correctAnswer": "$ran(A) = m$",
              "explanation": "Una matriz tiene inversa a derecha si su rango es máximo por filas ($ran(A)=m$), lo que garantiza que el sistema $Ax=b$ sea compatible para cualquier $b∈ℝᵐ$."
            },
            {
              "id": "AL_INT_015",
              "subCategory": "Inversa de Matrices",
              "text": "Si $A$ y $B$ son matrices invertibles, ¿a qué es igual la inversa de su producto $(A \\cdot B)^{-1}$?",
              "hint": "Verifica la propiedad mediante multiplicación directa para ver el orden correcto.",
              "options": [
                "$A^{-1} \\cdot B^{-1}$",
                "$B^{-1} \\cdot A^{-1}$",
                "$A \\cdot B$",
                "$(B \\cdot A)^{-1}$"
              ],
              "correctAnswer": "$B^{-1} \\cdot A^{-1}$",
              "explanation": "La inversa del producto es el producto de las inversas en orden inverso: $(A⋅B)^{-1}=B^{-1}⋅A^{-1}$. Esto se comprueba multiplicando: $(A⋅B)(B^{-1}⋅A^{-1})=I$."
            }
        ],
        "avanzado": [
          
            {
              "id": "AL_AVA_001",
              "subCategory": "Determinantes",
              "text": "Si ( A in mathcal{M}_n(mathbb{R}) ) es una matriz idempotente (( A^2 = A )), ¿qué valor(es) puede tomar ( det(A) )?",
              "hint": "Usa la propiedad del determinante para matrices que cumplen ( A^k = A ).",
              "options": ["Solo 1", "Solo 0", "0 o 1", "Cualquier número real"],
              "correctAnswer": "0 o 1",
              "explanation": "Como ( A^2 = A ), tomando determinantes: ( det(A)^2 = det(A) ). Esto implica ( det(A)(det(A) - 1) = 0 ), por lo que ( det(A) = 0 ) o ( 1 )."
            },
            {
              "id": "AL_AVA_002",
              "subCategory": "Propiedades de Matrices",
              "text": "Una matriz ( A in mathcal{M}_n(mathbb{R}) ) es normal si ( AA^T = A^T A ). ¿Cuál de estas matrices **no** es necesariamente normal?",
              "hint": "Piensa en matrices que no conmutan con su transpuesta, excepto en casos especiales.",
              "options": [
                "Matriz simétrica (( A = A^T ))",
                "Matriz antisimétrica (( A = -A^T ))",
                "Matriz ortogonal (( A^T = A^{-1} ))",
                "Matriz triangular superior no diagonal"
              ],
              "correctAnswer": "Matriz triangular superior no diagonal",
              "explanation": "Las matrices simétricas, antisimétricas y ortogonales siempre son normales. Una matriz triangular superior solo es normal si es diagonal."
            },
            {
              "id": "AL_AVA_003",
              "subCategory": "Propiedades de Matrices",
              "text": "Si ( A in mathcal{M}_n(mathbb{R}) ) es involutiva (( A^2 = I_n )), ¿qué relación tiene con su inversa ( A^{-1} )?",
              "hint": "Despeja ( A^{-1} ) de la definición de matriz involutiva.",
              "options": [
                "( A^{-1} = I_n )",
                "( A^{-1} = A )",
                "( A^{-1} = -A )",
                "( A^{-1} = A^T )"
              ],
              "correctAnswer": "( A^{-1} = A )",
              "explanation": "De ( A^2 = I_n ), multiplicando por ( A^{-1} ): ( A = A^{-1} ). Ejemplo clásico: matrices de reflexión."
            },
            {
              "id": "AL_AVA_004",
              "subCategory": "Sistemas de Ecuaciones Lineales",
              "text": "Según el Teorema de Rouché-Fröbenius, un sistema ( Amathbf{x} = mathbf{b} ) es compatible determinado si y solo si:",
              "hint": "Considera la relación entre el rango de ( A ), el de la matriz ampliada ( [A|mathbf{b}] ), y el número de incógnitas ( n ).",
              "options": [
                "( \text{rg}(A) \neq \text{rg}([A|mathbf{b}]) )",
                "( \text{rg}(A) = \text{rg}([A|mathbf{b}]) < n )",
                "( \text{rg}(A) = \text{rg}([A|mathbf{b}]) = n )",
                "( \text{rg}(A) = n )"
              ],
              "correctAnswer": "( \text{rg}(A) = \text{rg}([A|mathbf{b}]) = n )",
              "explanation": "El sistema tiene solución única (determinado) si y solo si el rango de ( A ) y de la matriz ampliada coinciden y son iguales al número de incógnitas."
            },
            {
              "id": "AL_AVA_005",
              "subCategory": "Pseudoinversa",
              "text": "Dada ( A in mathcal{M}_{m \times n}(mathbb{R}) ) con ( m geq n ) y ( \text{rg}(A) = n ), su pseudoinversa ( A^+ ) es:",
              "hint": "Usa la fórmula para pseudoinversa cuando ( A ) tiene columnas linealmente independientes.",
              "options": [
                "( A^+ = (A^T A)^{-1} A^T )",
                "( A^+ = A^T (A A^T)^{-1} )",
                "( A^+ = A^{-1} )",
                "( A^+ = A^T )"
              ],
              "correctAnswer": "( A^+ = (A^T A)^{-1} A^T )",
              "explanation": "Cuando ( \text{rg}(A) = n ), ( A^T A ) es invertible y ( A^+ ) se calcula así. Es la solución de mínimos cuadrados para sistemas sobredeterminados."
            },
            {
              "id": "AL_AVA_006",
              "subCategory": "Propiedades de Matrices",
              "text": "Una matriz ( A in mathcal{M}_n(mathbb{R}) ) es involutiva si cumple:",
              "hint": "Revisa la definición de matriz involutiva en términos de potencias.",
              "options": [
                "( A^2 = A )",
                "( A^2 = I_n )",
                "( A A^T = I_n )",
                "( A = -A^T )"
              ],
              "correctAnswer": "( A^2 = I_n )",
              "explanation": "Involutiva significa que ( A ) es su propia inversa al cuadrado. Ejemplo: ( A = \begin{pmatrix} 0 & 1 \\ 1 & 0 end{pmatrix} )."
            },
            {
              "id": "AL_AVA_007",
              "subCategory": "Sistemas de Ecuaciones Lineales",
              "text": "Si ( Amathbf{x} = mathbf{b} ) es compatible indeterminado, ¿qué se cumple?",
              "hint": "Piensa en sistemas con infinitas soluciones.",
              "options": [
                "( \text{rg}(A) = \text{rg}([A|mathbf{b}]) = n )",
                "( \text{rg}(A) = \text{rg}([A|mathbf{b}]) < n )",
                "( \text{rg}(A) \neq \text{rg}([A|mathbf{b}]) )",
                "( \text{rg}(A) = n )"
              ],
              "correctAnswer": "( \text{rg}(A) = \text{rg}([A|mathbf{b}]) < n )",
              "explanation": "Un sistema es indeterminado si los rangos son iguales pero menores que ( n ), indicando variables libres."
            },
            {
              "id": "AL_AVA_008",
              "subCategory": "Pseudoinversa",
              "text": "La pseudoinversa ( A^+ ) de una matriz ( A ):",
              "hint": "Considera propiedades de unicidad y casos especiales.",
              "options": [
                "Siempre es única",
                "Solo existe para matrices cuadradas",
                "Es igual a ( A^{-1} ) si ( det(A) = 0 )",
                "Se calcula siempre como ( (A^T A)^{-1} A^T )"
              ],
              "correctAnswer": "Siempre es única",
              "explanation": "La pseudoinversa de Moore-Penrose existe para cualquier matriz y es única, incluso si ( A ) no es invertible."
            },
            {
              "id": "AL_AVA_009",
              "subCategory": "Propiedades de Matrices",
              "text": "Si ( A in mathcal{M}_n(mathbb{R}) ) es invertible, ¿qué afirmación es equivalente?",
              "hint": "Relaciona invertibilidad con propiedades de rango y determinante.",
              "options": [
                "( A ) es la matriz nula",
                "( det(A) = 0 )",
                "( A ) tiene inversa por la derecha",
                "( A^T = A )"
              ],
              "correctAnswer": "( A ) tiene inversa por la derecha",
              "explanation": "Una matriz es invertible si y solo si tiene inversa por ambos lados. Además, ( det(A) \neq 0 ) y ( \text{rg}(A) = n )."
            },
            {
              "id": "AL_AVA_010",
              "subCategory": "Operaciones Elementales",
              "text": "Para reducir una matriz a su forma escalonada reducida (RREF), el último paso es:",
              "hint": "Recuerda los pasos de eliminación de Gauss-Jordan.",
              "options": [
                "Hacer ceros debajo de los pivotes",
                "Escalar filas para que los pivotes sean 1",
                "Hacer ceros **encima** de los pivotes",
                "Calcular el determinante"
              ],
              "correctAnswer": "Hacer ceros **encima** de los pivotes",
              "explanation": "Tras escalonar y normalizar pivotes a 1, se anulan los elementos por encima de ellos para obtener la RREF."
            },
            {
              "id": "AL_AVA_011",
              "subCategory": "Pseudoinversa",
              "text": "Si ( A in mathcal{M}_{m \times n}(mathbb{R}) ) con ( m leq n ) y ( \text{rg}(A) = m ), su pseudoinversa ( A^+ ) es:",
              "hint": "Usa la fórmula para matrices con filas linealmente independientes.",
              "options": [
                "( A^+ = (A^T A)^{-1} A^T )",
                "( A^+ = A^T (A A^T)^{-1} )",
                "( A^+ = A^{-1} )",
                "( A^+ = (A A^T)^{-1} )"
              ],
              "correctAnswer": "( A^+ = A^T (A A^T)^{-1} )",
              "explanation": "Cuando ( \text{rg}(A) = m ), ( A A^T ) es invertible y ( A^+ ) se calcula así. Es útil en problemas subdeterminados."
            },
            {
              "id": "AL_AVA_012",
              "subCategory": "Sistemas de Ecuaciones",
              "text": "Un sistema lineal homogéneo ( Amathbf{x} = mathbf{0} ) se caracteriza por:",
              "hint": "Considera la estructura de sus soluciones.",
              "options": [
                "Tener solución única ( mathbf{x} = mathbf{0} )",
                "Tener términos independientes nulos",
                "Ser siempre incompatible",
                "Tener infinitas soluciones no triviales si ( det(A) \neq 0 )"
              ],
              "correctAnswer": "Tener términos independientes nulos",
              "explanation": "Por definición, un sistema homogéneo tiene ( mathbf{b} = mathbf{0} ). Siempre admite ( mathbf{x} = mathbf{0} ), pero puede tener infinitas soluciones si ( \text{rg}(A) < n )."
            },
            {
              "id": "AL_AVA_013",
              "subCategory": "Sistemas de Ecuaciones",
              "text": "Según Rouché-Fröbenius, ( Amathbf{x} = mathbf{b} ) es incompatible si:",
              "hint": "Incompatibilidad implica inconsistencia en los rangos.",
              "options": [
                "( \text{rg}(A) = \text{rg}([A|mathbf{b}]) = n )",
                "( \text{rg}(A) = \text{rg}([A|mathbf{b}]) < n )",
                "( \text{rg}(A) \neq \text{rg}([A|mathbf{b}]) )",
                "( \text{rg}(A) = n ) y ( \text{rg}([A|mathbf{b}]) = n+1 )"
              ],
              "correctAnswer": "( \text{rg}(A) \neq \text{rg}([A|mathbf{b}]) )",
              "explanation": "Un sistema es incompatible si el rango de ( A ) difiere del de la matriz ampliada, indicando que ( mathbf{b} ) no está en el espacio columna de ( A )."
            },
            {
              "id": "AL_AVA_014",
              "subCategory": "Pseudoinversa",
              "text": "Si ( A ) es cuadrada e invertible, su pseudoinversa ( A^+ ) coincide con:",
              "hint": "Compara la pseudoinversa con la inversa clásica.",
              "options": [
                "( A^T )",
                "( A^{-1} )",
                "La matriz nula",
                "La matriz identidad"
              ],
              "correctAnswer": "( A^{-1} )",
              "explanation": "Para matrices invertibles, ( A^+ = A^{-1} ), ya que satisface todas las condiciones de la pseudoinversa."
            },
            {
              "id": "AL_AVA_015",
              "subCategory": "Sistemas de Ecuaciones",
              "text": "En un sistema homogéneo ( Amathbf{x} = mathbf{0} ), ¿qué solución siempre existe?",
              "hint": "Piensa en la solución trivial.",
              "options": [
                "El vector de unos ( (1, 1, dots, 1)^T )",
                "El vector nulo ( mathbf{0} )",
                "No hay soluciones garantizadas",
                "Solo soluciones no nulas si ( det(A) = 0 )"
              ],
              "correctAnswer": "El vector nulo ( mathbf{0} )",
              "explanation": "El vector ( mathbf{0} ) siempre es solución de ( Amathbf{x} = mathbf{0} ). Las soluciones no triviales existen si ( \text{rg}(A) < n )."
            } 
        ]
      }
    },
    {
      "id": "algebra_lineal_2",
      "name": "Álgebra Lineal II",
      "description": "Preguntas sobre espacios vectoriales, aplicaciones lineales y conceptos avanzados.",
      "difficultyLevels": {
        "basico": [
            {
              "id": "AL_EASY_001",
              "subCategory": "Espacios Vectoriales",
              "text": "¿Cuál es la terna que define formalmente un espacio vectorial real?",
              "hint": "Considera las operaciones básicas que debe admitir un espacio vectorial.",
              "options": [
                "$(V, \\times, \\div)$",
                "$(V, +, \\cdot)$",
                "$(V, \\oplus, \\otimes)$",
                "$(V, \\cap, \\cup)$"
              ],
              "correctAnswer": "$(V, +, \\cdot)$",
              "explanation": "Un espacio vectorial real se define por el conjunto $V$, una operación de suma $(+)$ entre vectores y una operación de producto $(\\cdot)$ por escalares reales, cumpliendo los 8 axiomas de espacios vectoriales."
            },
            {
              "id": "AL_EASY_002",
              "subCategory": "Conceptos Fundamentales",
              "text": "En $\\mathbb{R}^3$, ¿qué nombre reciben sus elementos?",
              "hint": "Piensa en la naturaleza geométrica de los elementos.",
              "options": ["Escalares", "Matrices", "Vectores", "Tensores"],
              "correctAnswer": "Vectores",
              "explanation": "En cualquier espacio vectorial, incluido $\\mathbb{R}^3$, los elementos se denominan vectores. En $\\mathbb{R}^3$ pueden interpretarse geométricamente como flechas desde el origen."
            },
            {
              "id": "AL_EASY_003",
              "subCategory": "Propiedades del Espacio Vectorial",
              "text": "Sobre el vector nulo $\\mathbf{0}$ en $V$, ¿cuál afirmación es verdadera?",
              "hint": "Considera la unicidad en estructuras algebraicas.",
              "options": [
                "No es único",
                "Es único y satisface $\\forall \\mathbf{v} \\in V, \\mathbf{v} + \\mathbf{0} = \\mathbf{v}$",
                "Depende de la base elegida",
                "Solo existe en $\\mathbb{R}^n$"
              ],
              "correctAnswer": "Es único y satisface $\\forall \\mathbf{v} \\in V, \\mathbf{v} + \\mathbf{0} = \\mathbf{v}$",
              "explanation": "El vector nulo en un espacio vectorial es único y cumple ser el elemento neutro de la suma vectorial. Esto se deduce de los axiomas de espacios vectoriales."
            },
            {
              "id": "AL_EASY_004",
              "subCategory": "Subespacios Vectoriales",
              "text": "Para que $W \\subset V$ sea subespacio, debe cumplir:",
              "hint": "Considera cerradura bajo operaciones vectoriales.",
              "options": [
                "$\\mathbf{0} \\notin W$",
                "$\\forall \\mathbf{u}, \\mathbf{v} \\in W, \\mathbf{u} + \\mathbf{v} \\notin W$",
                "$\\forall \\alpha \\in \\mathbb{R}, \\forall \\mathbf{u} \\in W, \\alpha \\mathbf{u} \\in W$",
                "$W$ debe ser finito"
              ],
              "correctAnswer": "$\\forall \\alpha \\in \\mathbb{R}, \\forall \\mathbf{u} \\in W, \\alpha \\mathbf{u} \\in W$",
              "explanation": "Un subespacio debe ser cerrado bajo suma vectorial y multiplicación por escalares, y contener al vector nulo. La opción correcta refleja la cerradura bajo producto por escalares."
            },
            {
              "id": "AL_EASY_005",
              "subCategory": "Espacio Afín",
              "text": "En un espacio afín $(E, V, +)$, los elementos de $E$ se llaman:",
              "hint": "Piensa en la interpretación geométrica.",
              "options": [
                "Vectores",
                "Puntos",
                "Escalares",
                "Transformaciones"
              ],
              "correctAnswer": "Puntos",
              "explanation": "En un espacio afín, $E$ es el conjunto de puntos y $V$ el espacio vectorial asociado que permite definir diferencias entre puntos y traslaciones."
            },
            {
              "id": "AL_EASY_006",
              "subCategory": "Distancia Euclídea",
              "text": "La distancia euclidiana entre $P, Q \\in \\mathbb{R}^n$ es:",
              "hint": "Usa la norma del vector diferencia.",
              "options": [
                "$\\|P\\| + \\|Q\\|$",
                "$\\|P - Q\\|$",
                "$P \\cdot Q$",
                "$\\det(P, Q)$"
              ],
              "correctAnswer": "$\\|P - Q\\|$",
              "explanation": "La distancia se define como la norma del vector $\\overrightarrow{PQ} = Q - P$, que en $\\mathbb{R}^n$ es $\\sqrt{\\sum_{i=1}^n (q_i - p_i)^2}$."
            },
            {
              "id": "AL_EASY_007",
              "subCategory": "Aplicaciones Lineales",
              "text": "Una transformación lineal $T: V \\to W$ debe cumplir:",
              "hint": "Busca preservación de operaciones.",
              "options": [
                "$T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) \\circ T(\\mathbf{v})$",
                "$T(\\alpha \\mathbf{u}) = \\alpha^2 T(\\mathbf{u})$",
                "$T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})$",
                "$T(\\mathbf{0}_V) = \\mathbf{1}_W$"
              ],
              "correctAnswer": "$T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})$",
              "explanation": "Las transformaciones lineales preservan suma y producto por escalar: $T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})$ y $T(\\alpha \\mathbf{u}) = \\alpha T(\\mathbf{u})$."
            },
            {
              "id": "AL_EASY_008",
              "subCategory": "Aplicaciones Lineales",
              "text": "Para toda transformación lineal $T: V \\to W$, se cumple:",
              "hint": "Considera la imagen del elemento neutro.",
              "options": [
                "$T(\\mathbf{0}_V) = \\mathbf{0}_W$",
                "$T(\\mathbf{0}_V) = \\mathbf{1}_W$",
                "$T(\\mathbf{0}_V)$ es indefinido",
                "$T(\\mathbf{0}_V) = \\dim(V)$"
              ],
              "correctAnswer": "$T(\\mathbf{0}_V) = \\mathbf{0}_W$",
              "explanation": "Toda transformación lineal mapea el vector nulo de $V$ al vector nulo de $W$, pues $T(\\mathbf{0}_V) = T(0 \\cdot \\mathbf{v}) = 0 \\cdot T(\\mathbf{v}) = \\mathbf{0}_W$."
            },
            {
              "id": "AL_EASY_009",
              "subCategory": "Variedades Lineales Afines",
              "text": "Una recta en un espacio afín es una variedad de dimensión:",
              "hint": "Piensa en los grados de libertad.",
              "options": ["0", "1", "2", "$n$"],
              "correctAnswer": "1",
              "explanation": "Una recta tiene dimensión 1, pues puede parametrizarse con un único parámetro: $P = P_0 + t \\mathbf{v}$, donde $t \\in \\mathbb{R}$."
            },
            {
              "id": "AL_EASY_010",
              "subCategory": "Dependencia Lineal",
              "text": "Si $\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\}$ contiene a $\\mathbf{0}$:",
              "hint": "Considera combinaciones lineales no triviales.",
              "options": [
                "Es linealmente independiente",
                "Es base de $V$",
                "Es linealmente dependiente",
                "Genera todo $V$"
              ],
              "correctAnswer": "Es linealmente dependiente",
              "explanation": "El conjunto es LD pues existe una combinación lineal no trivial $1 \\cdot \\mathbf{0} + 0 \\cdot \\mathbf{v}_2 + \\cdots + 0 \\cdot \\mathbf{v}_k = \\mathbf{0}$."
            },
            {
              "id": "AL_EASY_011",
              "subCategory": "Bases",
              "text": "La base canónica de $\\mathbb{R}^3$ es:",
              "hint": "Vectores unitarios en ejes coordenados.",
              "options": [
                "$\\{(1,1,1), (0,1,1), (0,0,1)\\}$",
                "$\\{(1,0,0), (0,1,0), (0,0,1)\\}$",
                "$\\{(2,0,0), (0,2,0), (0,0,2)\\}$",
                "$\\mathbb{R}^3$ no tiene base canónica"
              ],
              "correctAnswer": "$\\{(1,0,0), (0,1,0), (0,0,1)\\}$",
              "explanation": "La base canónica son los vectores unitarios en las direcciones de los ejes $x$, $y$ y $z$."
            },
            {
              "id": "AL_EASY_012",
              "subCategory": "Dimensión",
              "text": "Un punto en un espacio afín tiene dimensión:",
              "hint": "Grados de libertad para moverse.",
              "options": ["0", "1", "2", "3"],
              "correctAnswer": "0",
              "explanation": "Un punto es una variedad de dimensión 0, pues no permite movimiento en ninguna dirección."
            },
            {
              "id": "AL_EASY_013",
              "subCategory": "Sistemas de Referencia",
              "text": "En $\\mathcal{R} = \\{O; \\mathbf{e}_1, \\mathbf{e}_2\\}$, $O$ representa:",
              "hint": "Elemento que fija la posición.",
              "options": [
                "Un vector de la base",
                "El origen de coordenadas",
                "Un escalar",
                "La dimensión del espacio"
              ],
              "correctAnswer": "El origen de coordenadas",
              "explanation": "$O$ es el punto de referencia que sirve como origen del sistema de coordenadas."
            },
            {
              "id": "AL_EASY_014",
              "subCategory": "Núcleo e Imagen",
              "text": "Para $T: V \\to W$ lineal, el núcleo $\\ker(T)$ es:",
              "hint": "Conjunto de vectores que van al cero.",
              "options": [
                "$\\{\\mathbf{v} \\in V \\ | \\ T(\\mathbf{v}) = \\mathbf{0}_W\\}$",
                "$\\{\\mathbf{w} \\in W \\ | \\ \\exists \\mathbf{v} \\in V: T(\\mathbf{v}) = \\mathbf{w}\\}$",
                "$V$",
                "$W$"
              ],
              "correctAnswer": "$\\{\\mathbf{v} \\in V \\ | \\ T(\\mathbf{v}) = \\mathbf{0}_W\\}$",
              "explanation": "El núcleo son todos los vectores de $V$ que $T$ mapea al vector nulo de $W$. Es un subespacio de $V$."
            },
            {
              "id": "AL_EASY_015",
              "subCategory": "Coordenadas",
              "text": "Las coordenadas de $\\mathbf{v} = (2,3)$ en la base $\\mathcal{B} = \\{(1,1), (0,1)\\}$ son:",
              "hint": "Resuelve $\\mathbf{v} = \\alpha \\mathbf{b}_1 + \\beta \\mathbf{b}_2$.",
              "options": ["$(2, 1)$", "$(3, 2)$", "$(2, 3)$", "$(-1, 2)$"],
              "correctAnswer": "$(2, 1)$",
              "explanation": "Se resuelve $2 = \\alpha \\cdot 1 + \\beta \\cdot 0$ y $3 = \\alpha \\cdot 1 + \\beta \\cdot 1$, obteniendo $\\alpha = 2$, $\\beta = 1$."
            },
            {
              "id": "AL_EASY_016",
              "subCategory": "Producto Interno",
              "text": "En $\\mathbb{R}^n$, el producto interno estándar de $\\mathbf{u}$ y $\\mathbf{v}$ es:",
              "hint": "Suma de productos componente a componente.",
              "options": [
                "$\\sum_{i=1}^n u_i v_i$",
                "$\\sum_{i=1}^n |u_i - v_i|$",
                "$\\max(u_i v_i)$",
                "$\\prod_{i=1}^n u_i + v_i$"
              ],
              "correctAnswer": "$\\sum_{i=1}^n u_i v_i$",
              "explanation": "El producto interno estándar (o producto punto) es $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\sum_{i=1}^n u_i v_i$."
            },
            {
              "id": "AL_EASY_017",
              "subCategory": "Ortogonalidad",
              "text": "Dos vectores $\\mathbf{u}, \\mathbf{v}$ son ortogonales si:",
              "hint": "Condición del producto interno.",
              "options": [
                "$\\|\\mathbf{u}\\| = \\|\\mathbf{v}\\|$",
                "$\\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0$",
                "$\\mathbf{u} = \\alpha \\mathbf{v}$ para algún $\\alpha$",
                "$\\mathbf{u} + \\mathbf{v} = \\mathbf{0}$"
              ],
              "correctAnswer": "$\\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0$",
              "explanation": "La ortogonalidad se define mediante el producto interno: $\\mathbf{u} \\perp \\mathbf{v} \\iff \\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0$."
            },
            {
              "id": "AL_EASY_018",
              "subCategory": "Norma",
              "text": "La norma euclidiana de $\\mathbf{v} \\in \\mathbb{R}^n$ es:",
              "hint": "Relación con el producto interno.",
              "options": [
                "$\\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}$",
                "$\\langle \\mathbf{v}, \\mathbf{v} \\rangle$",
                "$\\sum_{i=1}^n v_i$",
                "$\\max(v_i)$"
              ],
              "correctAnswer": "$\\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}$",
              "explanation": "La norma euclidiana es $\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + \\cdots + v_n^2} = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}$."
            },
            {
              "id": "AL_EASY_019",
              "subCategory": "Matriz de Transformación",
              "text": "Si $T: \\mathbb{R}^2 \\to \\mathbb{R}^2$ es lineal con $T(1,0) = (a,b)$ y $T(0,1) = (c,d)$, su matriz asociada es:",
              "hint": "Columnas como imágenes de la base canónica.",
              "options": [
                "$\\begin{pmatrix} a & c \\\\ b & d \\end{pmatrix}$",
                "$\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$",
                "$\\begin{pmatrix} b & a \\\\ d & c \\end{pmatrix}$",
                "$\\begin{pmatrix} c & a \\\\ d & b \\end{pmatrix}$"
              ],
              "correctAnswer": "$\\begin{pmatrix} a & c \\\\ b & d \\end{pmatrix}$",
              "explanation": "Las columnas de la matriz de $T$ son las imágenes de los vectores de la base canónica: $[T] = \\begin{pmatrix} a & c \\\\ b & d \\end{pmatrix}$."
            },
            {
              "id": "AL_EASY_020",
              "subCategory": "Determinante",
              "text": "El determinante de una matriz $2 \\times 2$ $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ es:",
              "hint": "Producto diagonal menos producto antidiagonal.",
              "options": ["$ab + cd$", "$ad - bc$", "$ac - bd$", "$a + d$"],
              "correctAnswer": "$ad - bc$",
              "explanation": "Para matrices $2 \\times 2$, $\\det \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = ad - bc$."
            }
        ],
        "intermedio": [
            {
              "id": "AL_MEDIUM_001",
              "subCategory": "Independencia Lineal",
              "text": "Un conjunto $\\{\\mathbf{v}_1,...,\\mathbf{v}_m\\} \\subset \\mathbb{R}^n$ es linealmente independiente (LI) si:",
              "hint": "Considera la única combinación lineal que da el vector nulo.",
              "options": [
                "$\\exists \\alpha_i \\neq 0$ tal que $\\sum \\alpha_i \\mathbf{v}_i = \\mathbf{0}$",
                "La única solución para $\\sum \\alpha_i \\mathbf{v}_i = \\mathbf{0}$ es $\\alpha_i = 0$ $\\forall i$",
                "$\\sum \\alpha_i = 1$ para alguna combinación lineal",
                "$\\mathbf{v}_i = k\\mathbf{v}_j$ para algún $i,j$"
              ],
              "correctAnswer": "La única solución para $\\sum \\alpha_i \\mathbf{v}_i = \\mathbf{0}$ es $\\alpha_i = 0$ $\\forall i$",
              "explanation": "Por definición, un conjunto es LI si la única combinación lineal que produce el vector nulo es la trivial (todos los coeficientes cero). Esto garantiza que ningún vector puede expresarse como combinación de los demás."
            },
            {
              "id": "AL_MEDIUM_002",
              "subCategory": "Rango de una Matriz",
              "text": "Si $A = [\\mathbf{v}_1 | \\cdots | \\mathbf{v}_m] \\in \\mathcal{M}_{n \\times m}(\\mathbb{R})$, entonces $\\{\\mathbf{v}_1,...,\\mathbf{v}_m\\}$ es LI si y solo si:",
              "hint": "Relaciona independencia lineal con el rango matricial.",
              "options": [
                "$\\text{rg}(A) < m$",
                "$\\text{rg}(A) = m$",
                "$\\text{rg}(A) = n$",
                "$\\text{rg}(A) = 0$"
              ],
              "correctAnswer": "$\\text{rg}(A) = m$",
              "explanation": "El rango de $A$ es igual al máximo número de columnas LI. Si $\\text{rg}(A) = m$ (número total de columnas), todas las columnas son LI. Esto también implica $m \\leq n$."
            },
            {
              "id": "AL_MEDIUM_003",
              "subCategory": "Dimensión de Espacios Vectoriales",
              "text": "La dimensión de $\\mathbb{R}^n$ como espacio vectorial real es:",
              "hint": "Piensen en la base canónica.",
              "options": ["$n^2$", "$n$", "$2n$", "Depende de la base elegida"],
              "correctAnswer": "$n$",
              "explanation": "La dimensión es el cardinal de cualquier base. La base canónica $\\{\\mathbf{e}_1,...,\\mathbf{e}_n\\}$ tiene $n$ elementos, por tanto $\\dim(\\mathbb{R}^n) = n$."
            },
            {
              "id": "AL_MEDIUM_004",
              "subCategory": "Subespacios Vectoriales",
              "text": "Para subespacios $L_1, L_2 \\subseteq V$, la fórmula de Grassmann establece que:",
              "hint": "Considera la relación entre suma e intersección de subespacios.",
              "options": [
                "$\\dim(L_1 + L_2) = \\dim(L_1) + \\dim(L_2) + \\dim(L_1 \\cap L_2)$",
                "$\\dim(L_1 + L_2) = \\dim(L_1) + \\dim(L_2) - \\dim(L_1 \\cap L_2)$",
                "$\\dim(L_1 + L_2) = \\dim(L_1) \\cdot \\dim(L_2)$",
                "$\\dim(L_1 + L_2) = \\max(\\dim(L_1), \\dim(L_2))$"
              ],
              "correctAnswer": "$\\dim(L_1 + L_2) = \\dim(L_1) + \\dim(L_2) - \\dim(L_1 \\cap L_2)$",
              "explanation": "La fórmula de Grassmann cuenta la dimensión de la suma corrigiendo por la intersección para evitar 'contar dos veces' los vectores comunes. Es análoga al principio de inclusión-exclusión en teoría de conjuntos."
            },
            {
              "id": "AL_MEDIUM_005",
              "subCategory": "Suma Directa",
              "text": "La suma $L_1 \\oplus L_2$ es directa cuando:",
              "hint": "Piensa en cómo se intersectan los subespacios.",
              "options": [
                "$L_1 \\cap L_2$ es infinito-dimensional",
                "$L_1 \\cap L_2 = \\{\\mathbf{0}\\}$",
                "$L_1 \\subseteq L_2$",
                "$\\dim(L_1) = \\dim(L_2)$"
              ],
              "correctAnswer": "$L_1 \\cap L_2 = \\{\\mathbf{0}\\}$",
              "explanation": "La suma es directa cuando los subespacios sólo comparten el vector nulo, lo que garantiza que todo vector en $L_1 + L_2$ se expresa **de forma única** como suma de vectores de $L_1$ y $L_2$."
            },
            {
              "id": "AL_MEDIUM_006",
              "subCategory": "Cambios de Base",
              "text": "La matriz de cambio de base de $\\mathcal{B}_1$ a $\\mathcal{B}_2$:",
              "hint": "¿Qué hace esta matriz con las coordenadas?",
              "options": [
                "Multiplica por el vector nulo",
                "Transforma coordenadas de $\\mathcal{B}_1$ a $\\mathcal{B}_2$",
                "Es siempre la matriz identidad",
                "Depende solo de la dimensión"
              ],
              "correctAnswer": "Transforma coordenadas de $\\mathcal{B}_1$ a $\\mathcal{B}_2$",
              "explanation": "Si $[\\mathbf{v}]_{\\mathcal{B}_1}$ son las coordenadas en $\\mathcal{B}_1$, entonces $P_{\\mathcal{B}_1 \\to \\mathcal{B}_2} [\\mathbf{v}]_{\\mathcal{B}_1} = [\\mathbf{v}]_{\\mathcal{B}_2}$. Sus columnas son los vectores de $\\mathcal{B}_1$ expresados en $\\mathcal{B}_2$."
            },
            {
              "id": "AL_MEDIUM_007",
              "subCategory": "Sistemas Homogéneos",
              "text": "Para un sistema homogéneo $A\\mathbf{x} = \\mathbf{0}$ con $A \\in \\mathcal{M}_{m \\times n}(\\mathbb{R})$ y $\\text{rg}(A) = r$, la dimensión del espacio solución es:",
              "hint": "Usa el teorema de la dimensión para sistemas lineales.",
              "options": ["$r$", "$n - r$", "$m - r$", "$n$"],
              "correctAnswer": "$n - r$",
              "explanation": "Por el teorema de rango-nulidad, $\\dim(\\ker(A)) = n - \\text{rg}(A)$. Cada ecuación LI reduce en 1 la dimensión del espacio solución."
            },
            {
              "id": "AL_MEDIUM_008",
              "subCategory": "Aplicaciones Lineales",
              "text": "Una aplicación lineal $f: V \\to W$ con matriz asociada $A$ es inyectiva si:",
              "hint": "Relaciona inyectividad con el núcleo.",
              "options": [
                "$\\text{rg}(A) = \\dim(V)$",
                "$\\ker(f) = \\{\\mathbf{0}\\}$",
                "$\\text{rg}(A) = \\dim(W)$",
                "$A$ es cuadrada"
              ],
              "correctAnswer": "$\\ker(f) = \\{\\mathbf{0}\\}$",
              "explanation": "$f$ es inyectiva $\\iff \\ker(f) = \\{\\mathbf{0}\\} \\iff \\text{rg}(A) = \\dim(V)$. Esto significa que columnas de $A$ son LI."
            },
            {
              "id": "AL_MEDIUM_009",
              "subCategory": "Imagen de Subespacios",
              "text": "Si $H \\subseteq V$ es subespacio y $f: V \\to W$ es lineal, entonces $f(H)$:",
              "hint": "¿Preserva $f$ la estructura de subespacio?",
              "options": [
                "Es siempre $\\{\\mathbf{0}\\}$",
                "Es un subespacio de $W$",
                "No tiene estructura algebraica",
                "Es igual a $W$"
              ],
              "correctAnswer": "Es un subespacio de $W$",
              "explanation": "Las aplicaciones lineales preservan subespacios: $f(H)$ hereda cerradura bajo suma y producto por escalar de $W$. Si $H = \\langle \\mathbf{v}_1,...,\\mathbf{v}_k \\rangle$, entonces $f(H) = \\langle f(\\mathbf{v}_1),...,f(\\mathbf{v}_k) \\rangle$."
            },
            {
              "id": "AL_MEDIUM_010",
              "subCategory": "Sistemas Generadores",
              "text": "Si $\\{\\mathbf{v}_1,...,\\mathbf{v}_k\\}$ genera $V$ y $f: V \\to W$ es lineal, entonces:",
              "hint": "¿Qué pasa con el span bajo transformaciones lineales?",
              "options": [
                "$\\{f(\\mathbf{v}_1),...,f(\\mathbf{v}_k)\\}$ genera $\\ker(f)$",
                "$\\{f(\\mathbf{v}_1),...,f(\\mathbf{v}_k)\\}$ genera $\\text{Im}(f)$",
                "Es LI en $W$",
                "No genera ningún espacio"
              ],
              "correctAnswer": "$\\{f(\\mathbf{v}_1),...,f(\\mathbf{v}_k)\\}$ genera $\\text{Im}(f)$",
              "explanation": "La imagen de un sistema generador bajo $f$ es un sistema generador de la imagen de $f$. Esto es clave para calcular $\\text{rg}(f) = \\dim(\\text{Im}(f))$."
            },
            {
              "id": "AL_MEDIUM_011",
              "subCategory": "Dependencia Lineal",
              "text": "Un conjunto $S \\subset V$ es linealmente dependiente (LD) si:",
              "hint": "¿Existe una combinación lineal no trivial igual a cero?",
              "options": [
                "$\\forall \\mathbf{v} \\in S, \\mathbf{v} = \\mathbf{0}$",
                "$\\exists \\mathbf{v} \\in S$ que es combinación lineal de los demás",
                "$S$ es vacío",
                "$S$ genera $V$"
              ],
              "correctAnswer": "$\\exists \\mathbf{v} \\in S$ que es combinación lineal de los demás",
              "explanation": "Equivalentemente: $S$ es LD $\\iff \\exists \\alpha_i$ no todos cero tal que $\\sum \\alpha_i \\mathbf{v}_i = \\mathbf{0}$. Basta que un vector sea combinación lineal de los demás."
            },
            {
              "id": "AL_MEDIUM_012",
              "subCategory": "Bases",
              "text": "Si $\\mathcal{B} = \\{\\mathbf{v}_1,...,\\mathbf{v}_n\\}$ es base de $V$, entonces:",
              "hint": "Caracteriza las propiedades de una base.",
              "options": [
                "Todo vector de $V$ tiene infinitas representaciones en $\\mathcal{B}$",
                "Todo vector de $V$ tiene una única representación como combinación lineal de $\\mathcal{B}$",
                "$\\mathcal{B}$ es LD",
                "$\\dim(V) < n$"
              ],
              "correctAnswer": "Todo vector de $V$ tiene una única representación como combinación lineal de $\\mathcal{B}$",
              "explanation": "Una base es un conjunto LI que genera $V$. La unicidad de la combinación lineal es crucial para definir coordenadas."
            },
            {
              "id": "AL_MEDIUM_013",
              "subCategory": "Sobreyectividad",
              "text": "Una aplicación lineal $f: V \\to W$ es sobreyectiva si:",
              "hint": "¿Cubre todo el espacio de llegada?",
              "options": [
                "$\\ker(f) = \\{\\mathbf{0}\\}$",
                "$\\text{Im}(f) = W$",
                "$\\dim(V) = \\dim(W)$",
                "$f$ es invertible"
              ],
              "correctAnswer": "$\\text{Im}(f) = W$",
              "explanation": "$f$ es sobreyectiva (epimorfismo) si todo elemento de $W$ es imagen de algún elemento de $V$, es decir, si $\\text{rg}(f) = \\dim(W)$."
            },
            {
              "id": "AL_MEDIUM_014",
              "subCategory": "Isomorfismos",
              "text": "Un isomorfismo entre $V$ y $W$ es una aplicación lineal:",
              "hint": "¿Qué propiedades debe tener para ser isomorfismo?",
              "options": [
                "Inyectiva pero no sobreyectiva",
                "Sobreyectiva pero no inyectiva",
                "Biyectiva",
                "No lineal"
              ],
              "correctAnswer": "Biyectiva",
              "explanation": "Un isomorfismo es un homomorfismo (lineal) biyectivo. Esto implica $\\dim(V) = \\dim(W)$ y existencia de inversa $f^{-1}$ también lineal."
            },
            {
              "id": "AL_MEDIUM_015",
              "subCategory": "Teorema de Rango-Nulidad",
              "text": "Para $f: V \\to W$ lineal, el teorema de rango-nulidad establece:",
              "hint": "Relaciona las dimensiones del núcleo y la imagen.",
              "options": [
                "$\\dim(V) = \\dim(\\ker(f)) \\cdot \\dim(\\text{Im}(f))$",
                "$\\dim(V) = \\dim(\\ker(f)) + \\dim(\\text{Im}(f))$",
                "$\\dim(W) = \\dim(\\ker(f)) + \\dim(\\text{Im}(f))$",
                "$\\dim(\\ker(f)) = \\dim(\\text{Im}(f))$"
              ],
              "correctAnswer": "$\\dim(V) = \\dim(\\ker(f)) + \\dim(\\text{Im}(f))$",
              "explanation": "Este teorema fundamental conecta las dimensiones del núcleo (nulidad) y la imagen (rango): $\\dim(V) = \\dim(\\ker(f)) + \\text{rg}(f)$."
            },
            {
              "id": "AL_MEDIUM_016",
              "subCategory": "Matriz de una Aplicación Lineal",
              "text": "Si $f: V \\to W$ tiene matriz $A$ respecto a bases $\\mathcal{B}_V$ y $\\mathcal{B}_W$, entonces las columnas de $A$ son:",
              "hint": "¿Qué representan las columnas?",
              "options": [
                "Los vectores de $\\mathcal{B}_V$ en $\\mathcal{B}_W$",
                "Las imágenes de los vectores de $\\mathcal{B}_V$ expresadas en $\\mathcal{B}_W$",
                "Los vectores del núcleo",
                "Las coordenadas de $\\mathcal{B}_W$ en $V$"
              ],
              "correctAnswer": "Las imágenes de los vectores de $\\mathcal{B}_V$ expresadas en $\\mathcal{B}_W$",
              "explanation": "La columna $j$-ésima de $A$ contiene las coordenadas de $f(\\mathbf{b}_j^V)$ en la base $\\mathcal{B}_W$, donde $\\mathbf{b}_j^V$ es el $j$-ésimo vector de $\\mathcal{B}_V$."
            },
            {
              "id": "AL_MEDIUM_017",
              "subCategory": "Endomorfismos",
              "text": "Un endomorfismo es una aplicación lineal:",
              "hint": "¿Qué espacios involucra?",
              "options": [
                "$f: V \\to W$ con $V \\neq W$",
                "$f: V \\to V$",
                "$f: V \\to \\mathbb{R}$",
                "$f: V \\to \\{\\mathbf{0}\\}$"
              ],
              "correctAnswer": "$f: V \\to V$",
              "explanation": "Un endomorfismo es una transformación lineal de un espacio en sí mismo. Si además es biyectivo, se llama automorfismo."
            },
            {
              "id": "AL_MEDIUM_018",
              "subCategory": "Formas Lineales",
              "text": "Una forma lineal es una aplicación lineal:",
              "hint": "¿A qué espacio llega?",
              "options": [
                "$f: V \\to W$ con $\\dim(W) > 1$",
                "$f: V \\to \\mathbb{R}$",
                "$f: V \\to V$",
                "$f: V \\to \\mathcal{M}_n(\\mathbb{R})$"
              ],
              "correctAnswer": "$f: V \\to \\mathbb{R}$",
              "explanation": "Una forma lineal es un funcional lineal, es decir, una transformación lineal que mapea vectores a escalares. El conjunto de todas ellas forma el espacio dual $V^*$."
            },
            {
              "id": "AL_MEDIUM_019",
              "subCategory": "Espacio Dual",
              "text": "Si $\\dim(V) = n$, entonces $\\dim(V^*) = $",
              "hint": "¿Cómo se relaciona con la dimensión de $V$?",
              "options": ["$n^2$", "$n$", "$2n$", "$0$"],
              "correctAnswer": "$n$",
              "explanation": "El espacio dual $V^* = \\mathcal{L}(V, \\mathbb{R})$ tiene la misma dimensión que $V$. Si $\\mathcal{B}$ es base de $V$, entonces la base dual $\\mathcal{B}^*$ tiene $n$ elementos."
            },
            {
              "id": "AL_MEDIUM_020",
              "subCategory": "Matriz Dual",
              "text": "La matriz de una forma lineal $f: V \\to \\mathbb{R}$ en una base $\\mathcal{B}$ es:",
              "hint": "¿Qué forma tiene una matriz de 1 fila?",
              "options": [
                "Una matriz cuadrada $n \\times n$",
                "Un vector fila en $\\mathbb{R}^n$",
                "Un vector columna en $\\mathbb{R}^n$",
                "La matriz identidad"
              ],
              "correctAnswer": "Un vector fila en $\\mathbb{R}^n$",
              "explanation": "Las formas lineales se representan por matrices fila. Si $\\mathcal{B} = \\{\\mathbf{b}_1,...,\\mathbf{b}_n\\}$, entonces $[f]_{\\mathcal{B}} = (f(\\mathbf{b}_1), ..., f(\\mathbf{b}_n)) \\in \\mathcal{M}_{1 \\times n}(\\mathbb{R})$."
            }
        ],
        "avanzado": [
            {
              "id": "AL_HARD_001",
              "subCategory": "Bases de Espacios Vectoriales",
              "text": "Según el Lema de Steinitz, si $\\{\\mathbf{v}_1,...,\\mathbf{v}_r\\}$ es un conjunto LI en $V$ con $\\dim(V) = n$ y $r < n$, ¿cuántos vectores se necesitan para completar una base de $V$?",
              "hint": "Considera la relación entre la dimensión del espacio y los vectores LI disponibles.",
              "options": ["$r$", "$n - r$", "$n + r$", "$n$"],
              "correctAnswer": "$n - r$",
              "explanation": "El Lema de Steinitz garantiza que cualquier conjunto LI puede extenderse a una base. Como $\\dim(V) = n$ y ya tenemos $r$ vectores LI, faltan $n - r$ vectores para completar la base."
            },
            {
              "id": "AL_HARD_002",
              "subCategory": "Coordenadas",
              "text": "Si $\\mathbf{v} = \\sum_{i=1}^n \\alpha_i \\mathbf{b}_i$ donde $\\mathcal{B} = \\{\\mathbf{b}_1,...,\\mathbf{b}_n\\}$ es base de $V$, entonces $\\alpha_i$ se llama:",
              "hint": "¿Cómo se denominan los coeficientes en una combinación lineal respecto a una base?",
              "options": [
                "Componente del espacio dual",
                "Coordenada $i$-ésima de $\\mathbf{v}$ en $\\mathcal{B}$",
                "Vector de proyección",
                "Escalar de contracción"
              ],
              "correctAnswer": "Coordenada $i$-ésima de $\\mathbf{v}$ en $\\mathcal{B}$",
              "explanation": "Los escalares $\\alpha_i$ son las coordenadas de $\\mathbf{v}$ en la base $\\mathcal{B}$. La notación $[\\mathbf{v}]_{\\mathcal{B}} = (\\alpha_1,...,\\alpha_n)^T$ representa este vector de coordenadas."
            },
            {
              "id": "AL_HARD_003",
              "subCategory": "Cambios de Base",
              "text": "Si $P_{\\mathcal{B}_1 \\to \\mathcal{B}_2}$ es la matriz de cambio de $\\mathcal{B}_1$ a $\\mathcal{B}_2$, entonces $P_{\\mathcal{B}_2 \\to \\mathcal{B}_1}$ es:",
              "hint": "Piensa en la operación inversa del cambio de base.",
              "options": [
                "$P_{\\mathcal{B}_1 \\to \\mathcal{B}_2}^T$",
                "$P_{\\mathcal{B}_1 \\to \\mathcal{B}_2}^{-1}$",
                "$-P_{\\mathcal{B}_1 \\to \\mathcal{B}_2}$",
                "$P_{\\mathcal{B}_1 \\to \\mathcal{B}_2}$"
              ],
              "correctAnswer": "$P_{\\mathcal{B}_1 \\to \\mathcal{B}_2}^{-1}$",
              "explanation": "La matriz de cambio de base es invertible y su inversa corresponde al cambio en sentido opuesto: $P_{\\mathcal{B}_2 \\to \\mathcal{B}_1} = (P_{\\mathcal{B}_1 \\to \\mathcal{B}_2})^{-1}$."
            },
            {
              "id": "AL_HARD_004",
              "subCategory": "Ecuaciones de Subespacios",
              "text": "Para un subespacio $S \\subseteq \\mathbb{R}^n$ con $\\dim(S) = r$, el número de ecuaciones implícitas LI es:",
              "hint": "Relaciona con el teorema del rango para sistemas lineales.",
              "options": ["$r$", "$n - r$", "$n$", "$r - n$"],
              "correctAnswer": "$n - r$",
              "explanation": "Cada ecuación implícita LI reduce en 1 la dimensión del espacio. Para obtener $\\dim(S) = r$ en $\\mathbb{R}^n$, se necesitan $n - r$ ecuaciones implícitas LI."
            },
            {
              "id": "AL_HARD_005",
              "subCategory": "Variedades Afines",
              "text": "Una variedad afín $S = P + L$ donde $L$ es subespacio director puede parametrizarse como:",
              "hint": "Considera la combinación del punto base y los generadores del subespacio.",
              "options": [
                "$Q = P \\times (\\sum \\alpha_i \\mathbf{v}_i)$",
                "$Q = P + \\sum \\alpha_i \\mathbf{v}_i$",
                "$Q = \\sum \\alpha_i (P + \\mathbf{v}_i)$",
                "$Q = P - \\sum \\alpha_i \\mathbf{v}_i$"
              ],
              "correctAnswer": "$Q = P + \\sum \\alpha_i \\mathbf{v}_i$",
              "explanation": "Toda variedad afín se expresa como un punto base $P$ más una combinación lineal de los vectores directores $\\{\\mathbf{v}_i\\}$ de $L$. Los $\\alpha_i$ son parámetros libres."
            },
            {
              "id": "AL_HARD_006",
              "subCategory": "Intersección de Variedades",
              "text": "Las ecuaciones implícitas de $S_1 \\cap S_2$ se obtienen mediante:",
              "hint": "¿Cómo se combinan los sistemas de ecuaciones?",
              "options": [
                "Sumando las ecuaciones paramétricas",
                "Resolviendo el sistema unión de sus ecuaciones implícitas",
                "Multiplicando sus matrices asociadas",
                "Calculando el producto vectorial"
              ],
              "correctAnswer": "Resolviendo el sistema unión de sus ecuaciones implícitas",
              "explanation": "La intersección corresponde a los puntos que satisfacen simultáneamente las ecuaciones de $S_1$ y $S_2$. Esto equivale a resolver el sistema combinado de todas las ecuaciones implícitas."
            },
            {
              "id": "AL_HARD_007",
              "subCategory": "Teorema de la Dimensión",
              "text": "Para variedades afines disjuntas $S_1 = P_1 + L_1$ y $S_2 = P_2 + L_2$, la dimensión de la variedad unión es:",
              "hint": "Considera la fórmula de Grassmann para subespacios directores.",
              "options": [
                "$\\dim(L_1) + \\dim(L_2) - \\dim(L_1 \\cap L_2) + 1$",
                "$\\dim(L_1) + \\dim(L_2)$",
                "$\\max(\\dim(L_1), \\dim(L_2))$",
                "$\\dim(L_1 \\cap L_2)$"
              ],
              "correctAnswer": "$\\dim(L_1) + \\dim(L_2) - \\dim(L_1 \\cap L_2) + 1$",
              "explanation": "La dimensión de $S_1 \\vee S_2$ es $\\dim(L_1 + L_2) + 1 = \\dim(L_1) + \\dim(L_2) - \\dim(L_1 \\cap L_2) + 1$, pues se añade un grado de libertad al ser disjuntas."
            },
            {
              "id": "AL_HARD_008",
              "subCategory": "Monomorfismos",
              "text": "Si $f: V \\to W$ es inyectiva y $\\{\\mathbf{v}_1,...,\\mathbf{v}_k\\}$ es LI en $V$, entonces $\\{f(\\mathbf{v}_1),...,f(\\mathbf{v}_k)\\}$ es:",
              "hint": "¿Cómo preservan los monomorfismos la independencia lineal?",
              "options": ["LD", "LI", "Generador de $W$", "Base de $W$"],
              "correctAnswer": "LI",
              "explanation": "Los monomorfismos (aplicaciones lineales inyectivas) preservan la independencia lineal. La prueba surge de la definición: si $\\sum \\alpha_i f(\\mathbf{v}_i) = \\mathbf{0}$, entonces $f(\\sum \\alpha_i \\mathbf{v}_i) = \\mathbf{0} \\implies \\sum \\alpha_i \\mathbf{v}_i = \\mathbf{0}$ por inyectividad, luego $\\alpha_i = 0$."
            },
            {
              "id": "AL_HARD_009",
              "subCategory": "Dimensión de Imágenes",
              "text": "Para $f: V \\to W$ lineal y $H \\leq V$, se cumple:",
              "hint": "Considera el teorema del rango aplicado a la restricción $f|_H$.",
              "options": [
                "$\\dim(f(H)) = \\dim(H)$",
                "$\\dim(f(H)) \\leq \\dim(H)$",
                "$\\dim(f(H)) > \\dim(H)$",
                "$\\dim(f(H)) = \\dim(W)$"
              ],
              "correctAnswer": "$\\dim(f(H)) \\leq \\dim(H)$",
              "explanation": "Como $f|_H: H \\to W$ es lineal, $\\dim(f(H)) = \\dim(H) - \\dim(\\ker(f) \\cap H) \\leq \\dim(H)$. La igualdad se da cuando $f$ es inyectiva en $H$."
            },
            {
              "id": "AL_HARD_010",
              "subCategory": "Matriz de Aplicación Lineal",
              "text": "Si $[f]_{\\mathcal{B}, \\mathcal{C}} = A$ y cambiamos a bases $\\mathcal{B}'$, $\\mathcal{C}'$, la nueva matriz es:",
              "hint": "Usa matrices de cambio de base en dominio y codominio.",
              "options": [
                "$P_{\\mathcal{C}' \\to \\mathcal{C}} A P_{\\mathcal{B} \\to \\mathcal{B}'}$",
                "$P_{\\mathcal{C} \\to \\mathcal{C}'}} A P_{\\mathcal{B}' \\to \\mathcal{B}}$",
                "$P_{\\mathcal{C} \\to \\mathcal{C}'}}^{-1} A P_{\\mathcal{B} \\to \\mathcal{B}'}$",
                "$A^{-1}$"
              ],
              "correctAnswer": "$P_{\\mathcal{C} \\to \\mathcal{C}'}}^{-1} A P_{\\mathcal{B} \\to \\mathcal{B}'}$",
              "explanation": "La matriz se transforma como $[f]_{\\mathcal{B}', \\mathcal{C}'} = P_{\\mathcal{C} \\to \\mathcal{C}'}^{-1} [f]_{\\mathcal{B}, \\mathcal{C}} P_{\\mathcal{B} \\to \\mathcal{B}'}$, donde $P$ son matrices de cambio de base."
            },
            {
              "id": "AL_HARD_011",
              "subCategory": "Rango por Menores",
              "text": "El rango de $A \\in \\mathcal{M}_{m \\times n}(\\mathbb{R})$ es el máximo $k$ tal que:",
              "hint": "Considera determinantes de submatrices cuadradas.",
              "options": [
                "Existe un menor $k \\times k$ con determinante no nulo",
                "Todos los menores $k \\times k$ tienen determinante cero",
                "Alguna fila es combinación lineal de las demás",
                "Las columnas forman un conjunto LI"
              ],
              "correctAnswer": "Existe un menor $k \\times k$ con determinante no nulo",
              "explanation": "Por el método de orlado, el rango es el mayor orden $k$ de un menor no nulo. Esto equivale al tamaño de la mayor submatriz cuadrada invertible."
            },
            {
              "id": "AL_HARD_012",
              "subCategory": "Espacio Afín",
              "text": "En un espacio afín $(A, V, +)$, el vector $\\overrightarrow{PQ}$ satisface:",
              "hint": "Considera las propiedades de los vectores de posición.",
              "options": [
                "$\\overrightarrow{PQ} = \\overrightarrow{QP}$",
                "$\\overrightarrow{PQ} + \\overrightarrow{QR} = \\overrightarrow{PR}$ (Regla del triángulo)",
                "$\\overrightarrow{PQ} = \\mathbf{0}$ para todo $P, Q$",
                "$\\overrightarrow{PQ} = P \\times Q$"
              ],
              "correctAnswer": "$\\overrightarrow{PQ} + \\overrightarrow{QR} = \\overrightarrow{PR}$ (Regla del triángulo)",
              "explanation": "Los vectores en espacios afines cumplen: 1) $\\overrightarrow{PQ} = -\\overrightarrow{QP}$, 2) $\\overrightarrow{PQ} + \\overrightarrow{QR} = \\overrightarrow{PR}$, y 3) $\\overrightarrow{PP} = \\mathbf{0}$."
            },
            {
              "id": "AL_HARD_013",
              "subCategory": "Isomorfismos",
              "text": "Si $f: V \\to W$ es un isomorfismo, entonces:",
              "hint": "Considera las propiedades de las aplicaciones biyectivas lineales.",
              "options": [
                "$\\dim(V) > \\dim(W)$",
                "$\\dim(V) = \\dim(W)$ y $[f]_{\\mathcal{B}, \\mathcal{C}}$ es cuadrada e invertible",
                "$\\ker(f) = W$",
                "$\\text{Im}(f) \\subsetneq W$"
              ],
              "correctAnswer": "$\\dim(V) = \\dim(W)$ y $[f]_{\\mathcal{B}, \\mathcal{C}}$ es cuadrada e invertible",
              "explanation": "Un isomorfismo es una aplicación lineal biyectiva, lo que requiere $\\dim(V) = \\dim(W)$. Su matriz en cualquier base es cuadrada e invertible, y preserva todas las estructuras lineales."
            },
            {
              "id": "AL_HARD_014",
              "subCategory": "Descomposición de Espacio",
              "text": "Si $V = U \\oplus W$, entonces para todo $\\mathbf{v} \\in V$:",
              "hint": "¿Cómo se expresan los vectores en una suma directa?",
              "options": [
                "$\\exists! \\mathbf{u} \\in U, \\mathbf{w} \\in W$ tales que $\\mathbf{v} = \\mathbf{u} + \\mathbf{w}$",
                "$\\mathbf{v} = \\mathbf{0}$ necesariamente",
                "La descomposición no es única",
                "$U \\cap W$ es infinito-dimensional"
              ],
              "correctAnswer": "$\\exists! \\mathbf{u} \\in U, \\mathbf{w} \\in W$ tales que $\\mathbf{v} = \\mathbf{u} + \\mathbf{w}$",
              "explanation": "En una suma directa, todo vector se descompone de forma única como suma de elementos de $U$ y $W$. Esto equivale a que $U \\cap W = \\{\\mathbf{0}\\}$."
            },
            {
              "id": "AL_HARD_015",
              "subCategory": "Teorema de Rango-Nulidad",
              "text": "Para $f: V \\to W$ lineal, el teorema de rango-nulidad establece que:",
              "hint": "Relaciona las dimensiones del núcleo y la imagen.",
              "options": [
                "$\\dim(V) = \\dim(\\ker(f)) \\cdot \\dim(\\text{Im}(f))$",
                "$\\dim(V) = \\dim(\\ker(f)) + \\dim(\\text{Im}(f))$",
                "$\\dim(W) = \\dim(\\ker(f)) + \\dim(\\text{Im}(f))$",
                "$\\dim(\\ker(f)) = \\dim(\\text{Im}(f))$"
              ],
              "correctAnswer": "$\\dim(V) = \\dim(\\ker(f)) + \\dim(\\text{Im}(f))$",
              "explanation": "Este teorema fundamental conecta las dimensiones: $\\dim(V) = \\dim(\\ker(f)) (\\text{nulidad}) + \\dim(\\text{Im}(f)) (\\text{rango})$. Es clave para analizar sistemas lineales."
            },
            {
              "id": "AL_HARD_016",
              "subCategory": "Subespacios Invariantes",
              "text": "Un subespacio $U \\subseteq V$ es $f$-invariante para $f: V \\to V$ si:",
              "hint": "¿Qué condición cumple la imagen de $U$ bajo $f$?",
              "options": [
                "$f(U) \\subseteq U$",
                "$f(U) = \\{\\mathbf{0}\\}$",
                "$f(U) \\cap U = \\emptyset$",
                "$f(U) = V$"
              ],
              "correctAnswer": "$f(U) \\subseteq U$",
              "explanation": "$U$ es invariante si $f$ mapea todo vector de $U$ nuevamente a $U$. Esto es crucial para descomposiciones de operadores y formas canónicas."
            },
            {
              "id": "AL_HARD_017",
              "subCategory": "Proyecciones",
              "text": "Un endomorfismo $p: V \\to V$ es una proyección si:",
              "hint": "Considera la propiedad idempotente.",
              "options": [
                "$p^2 = p$",
                "$p$ es invertible",
                "$\\ker(p) = \\text{Im}(p)$",
                "$p$ es nilpotente"
              ],
              "correctAnswer": "$p^2 = p$",
              "explanation": "Las proyecciones satisfacen $p^2 = p$ (idempotencia). Si $V = U \\oplus W$, entonces $p(\\mathbf{u} + \\mathbf{w}) = \\mathbf{u}$ es la proyección sobre $U$ paralela a $W$."
            },
            {
              "id": "AL_HARD_018",
              "subCategory": "Formas Canónicas",
              "text": "La forma de Jordan de una matriz $A$ existe cuando:",
              "hint": "Considera la condición sobre el cuerpo base.",
              "options": [
                "Todos los valores propios están en el cuerpo base",
                "$A$ es ortogonal",
                "$A$ tiene rango máximo",
                "El polinomio característico es irreducible"
              ],
              "correctAnswer": "Todos los valores propios están en el cuerpo base",
              "explanation": "La forma canónica de Jordan existe si el polinomio característico se descompone completamente en el cuerpo base. Esto garantiza suficientes vectores propios/generalizados."
            },
            {
              "id": "AL_HARD_019",
              "subCategory": "Descomposición QR",
              "text": "En la factorización $A = QR$ de $A \\in \\mathcal{M}_{m \\times n}(\\mathbb{R})$, $Q$ es:",
              "hint": "Considera las propiedades de ortogonalidad.",
              "options": [
                "Triangular superior",
                "Una matriz con columnas ortonormales",
                "Diagonal",
                "Simétrica"
              ],
              "correctAnswer": "Una matriz con columnas ortonormales",
              "explanation": "En la descomposición QR, $Q$ tiene columnas ortonormales (obtenidas por Gram-Schmidt) y $R$ es triangular superior. Es útil para resolver sistemas least-squares."
            },
            {
              "id": "AL_HARD_020",
              "subCategory": "Descomposición Espectral",
              "text": "Si $A \\in \\mathcal{M}_n(\\mathbb{R})$ es simétrica, entonces:",
              "hint": "Considera el teorema espectral.",
              "options": [
                "Es diagonalizable con autovectores ortogonales",
                "No tiene autovalores reales",
                "Su forma de Jordan es diagonal por bloques",
                "No es diagonalizable"
              ],
              "correctAnswer": "Es diagonalizable con autovectores ortogonales",
              "explanation": "El teorema espectral garantiza que toda matriz simétrica real es diagonalizable mediante una matriz ortogonal: $A = PDP^T$ con $D$ diagonal y $P$ ortogonal."
            }
        ]
      }
    },
    {
      "id": "algebra_lineal_3",
      "name": "Álgebra Lineal III",
      "description": "Preguntas sobre producto escalar, ortogonalidad, códigos lineales, factorización QR, y mínimos cuadrados.",
      "difficultyLevels": {
        "basico": [
            {
              "id": "AL_BAS_001",
              "subCategory": "Matrices",
              "text": "Dada la matriz $A = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$, ¿cuál de las siguientes propiedades **no** cumple?",
              "hint": "Piensa en operaciones que no preservan la estructura nula.",
              "options": [
                "$A + A = A$",
                "$A \\cdot A = A$",
                "$\\det(A) = 1$",
                "$A^T = A$"
              ],
              "correctAnswer": "$\\det(A) = 1$",
              "explanation": "La matriz nula tiene determinante cero (no 1). Sí cumple: $A + A = A$ (es idempotente para la suma), $A \\cdot A = A$ (idempotente para el producto), y $A^T = A$ (simétrica)."
            },
            {
              "id": "AL_BAS_002",
              "subCategory": "Matrices",
              "text": "Si $A$ es una matriz antisimétrica, ¿qué condición debe cumplir?",
              "hint": "Usa la propiedad de su transpuesta y considera los elementos diagonales.",
              "options": [
                "$A = A^T$",
                "$A = -A^T$ y los elementos diagonales son cero",
                "$A$ es diagonal",
                "$det(A) \\neq 0$"
              ],
              "correctAnswer": "$A = -A^T$ y los elementos diagonales son cero",
              "explanation": "Una matriz antisimétrica cumple $A = -A^T$. Además, los elementos diagonales deben ser cero porque $a_{ii} = -a_{ii} \\Rightarrow a_{ii} = 0$."
            },
            {
              "id": "AL_BAS_003",
              "subCategory": "Sistemas Lineales",
              "text": "¿Qué implica que un sistema $A\\mathbf{x} = \\mathbf{b}$ tenga infinitas soluciones?",
              "hint": "Considera el rango de la matriz ampliada y la matriz de coeficientes.",
              "options": [
                "$\\text{rango}(A) < \\text{rango}([A|\\mathbf{b}])$",
                "$\\text{rango}(A) = \\text{rango}([A|\\mathbf{b}]) < n$ (número de incógnitas)",
                "$\\det(A) \\neq 0$",
                "$\\mathbf{b} = \\mathbf{0}$"
              ],
              "correctAnswer": "$\\text{rango}(A) = \\text{rango}([A|\\mathbf{b}]) < n$ (número de incógnitas)",
              "explanation": "Un sistema tiene infinitas soluciones si el rango de $A$ es igual al de la matriz ampliada pero menor que el número de incógnitas (variables libres)."
            },
            {
              "id": "AL_BAS_004",
              "subCategory": "Espacios Vectoriales",
              "text": "¿Cuál es la dimensión del espacio generado por los vectores $\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}$ y $\\mathbf{v}_2 = \\begin{pmatrix} 2 \\\\ 0 \\\\ 4 \\end{pmatrix}$?",
              "hint": "Verifica si los vectores son linealmente independientes.",
              "options": ["0", "1", "2", "3"],
              "correctAnswer": "1",
              "explanation": "Los vectores son linealmente dependientes $\\mathbf{v}_2 = 2\\mathbf{v}_1$, por lo que generan una recta (dimensión 1)."
            },
            {
              "id": "AL_BAS_005",
              "subCategory": "Transformaciones Lineales",
              "text": "Si $T: \\mathbb{R}^2 \\to \\mathbb{R}^2$ es una transformación lineal con $T(1,0) = (2,3)$ y $T(0,1) = (-1,4)$, ¿cuál es su matriz asociada?",
              "hint": "Las columnas de la matriz son las imágenes de la base canónica.",
              "options": [
                "$\\begin{pmatrix} 2 & -1 \\\\ 3 & 4 \\end{pmatrix}$",
                "$\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$",
                "$\\begin{pmatrix} 2 & 3 \\\\ -1 & 4 \\end{pmatrix}$",
                "$\\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$"
              ],
              "correctAnswer": "$\\begin{pmatrix} 2 & -1 \\\\ 3 & 4 \\end{pmatrix}$",
              "explanation": "La matriz de $T$ se construye colocando como columnas las imágenes de los vectores de la base canónica: $[T] = \\begin{pmatrix} 2 & -1 \\\\ 3 & 4 \\end{pmatrix}$."
            },
            {
              "id": "AL_BAS_006",
              "subCategory": "Ortogonalidad",
              "text": "¿Cuál es la condición para que un conjunto de vectores $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}$ sea ortonormal?",
              "hint": "Combina ortogonalidad y norma.",
              "options": [
                "$\\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle = 1$ para todo $(i, j)$",
                "$\\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle = 0$ si $(i \\neq j)$ y $|\\mathbf{v}_i| = 1$ para todo $(i)$",
                "$|\\mathbf{v}_i| = 0$ para todo $(i)$",
                "Los vectores son linealmente dependientes"
              ],
              "correctAnswer": "$\\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle = 0$ si $(i \\neq j)$ y $|\\mathbf{v}_i| = 1$ para todo $(i)$",
              "explanation": "Un conjunto es ortonormal si los vectores son ortogonales entre sí ($\\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle = 0$) y cada uno tiene norma unitaria ($|\\mathbf{v}_i| = 1$)."
            },
            {
              "id": "AL_BAS_007",
              "subCategory": "Valores Propios",
              "text": "Si $\\lambda$ es un valor propio de $A$, ¿qué vector asociado existe?",
              "hint": "Recuerda la definición de valor propio y vector propio.",
              "options": [
                "Un vector $\\mathbf{v} \\neq \\mathbf{0}$ tal que $A\\mathbf{v} = \\lambda \\mathbf{v}$",
                "Un vector $\\mathbf{v} = \\mathbf{0}$",
                "Cualquier vector $\\mathbf{v}$ sin restricción",
                "Un vector $\\mathbf{v}$ tal que $A\\mathbf{v} = \\mathbf{0}$"
              ],
              "correctAnswer": "Un vector $\\mathbf{v} \\neq \\mathbf{0}$ tal que $A\\mathbf{v} = \\lambda \\mathbf{v}$",
              "explanation": "Un valor propio $\\lambda$ tiene asociado al menos un vector propio no nulo $\\mathbf{v}$ que cumple $A\\mathbf{v} = \\lambda \\mathbf{v}$."
            },
            {
              "id": "AL_BAS_008",
              "subCategory": "Diagonalización",
              "text": "¿Cuál es la condición necesaria y suficiente para que una matriz $A$ sea diagonalizable?",
              "hint": "Considera la multiplicidad algebraica y geométrica de los valores propios.",
              "options": [
                "Que tenga $n$ valores propios distintos",
                "Que la multiplicidad geométrica de cada valor propio sea igual a su multiplicidad algebraica",
                "Que $det(A) \\neq 0$",
                "Que $A$ sea simétrica"
              ],
              "correctAnswer": "Que la multiplicidad geométrica de cada valor propio sea igual a su multiplicidad algebraica",
              "explanation": "Una matriz es diagonalizable si y solo si para cada valor propio, su multiplicidad geométrica (dimensión del espacio propio) coincide con su multiplicidad algebraica (multiplicidad en el polinomio característico)."
            },
            {
              "id": "AL_BAS_009",
              "subCategory": "Producto Interno",
              "text": "En $\\mathbb{R}^3$, ¿cuál es el ángulo entre $\\mathbf{u} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ y $\\mathbf{v} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$?",
              "hint": "Usa la fórmula del coseno del ángulo entre vectores.",
              "options": [
                "$0^{\\circ}$",
                "$45^{\\circ}$",
                "$90^{\\circ}$",
                "$180^{\\circ}$"
              ],
              "correctAnswer": "$90^{\\circ}$",
              "explanation": "Como $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0$, los vectores son ortogonales y el ángulo es $90^{\\circ}$."
            },
            {
              "id": "AL_BAS_010",
              "subCategory": "Factorización QR",
              "text": "En la factorización $A = QR$, si $A$ es de $(m \\times n)$ con $(m > n)$, ¿qué propiedad tiene $Q$?",
              "hint": "Las columnas de $Q$ forman un conjunto ortonormal.",
              "options": [
                "$Q$ es triangular superior",
                "$Q$ tiene columnas ortonormales y es de $(m \\times n)$",
                "$Q$ es la matriz identidad",
                "$Q$ es de $(n \\times n)$"
              ],
              "correctAnswer": "$Q$ tiene columnas ortonormales y es de $(m \\times n)$",
              "explanation": "En la factorización QR reducida ($m > n$), $Q$ es $(m \\times n)$ con columnas ortonormales, y $R$ es $(n \\times n)$ triangular superior."
            },
            {
              "id": "AL_BAS_011",
              "subCategory": "Mínimos Cuadrados",
              "text": "Dado un sistema sobredeterminado ($A\\mathbf{x} = \\mathbf{b}$), ¿qué minimiza la solución de mínimos cuadrados?",
              "hint": "Considera la norma del residuo.",
              "options": [
                "$|A\\mathbf{x}|$",
                "$|\\mathbf{x}|$",
                "$|A\\mathbf{x} - \\mathbf{b}|^2$",
                "$\\det(A)$"
              ],
              "correctAnswer": "$|A\\mathbf{x} - \\mathbf{b}|^2$",
              "explanation": "El método de mínimos cuadrados encuentra $\\mathbf{x}$ que minimiza la norma cuadrática del residuo $|A\\mathbf{x} - \\mathbf{b}|^2$."
            },
            {
              "id": "AL_BAS_012",
              "subCategory": "Códigos Lineales",
              "text": "¿Qué propiedad debe cumplir la matriz de paridad $H$ de un código lineal para detectar $t$ errores?",
              "hint": "Relaciónala con la distancia mínima del código.",
              "options": [
                "Cualquier conjunto de $t$ columnas de $H$ debe ser linealmente independiente",
                "$H$ debe tener rango $t$",
                "$H$ debe ser cuadrada",
                "$H$ debe tener determinante cero"
              ],
              "correctAnswer": "Cualquier conjunto de $t$ columnas de $H$ debe ser linealmente independiente",
              "explanation": "Para detectar $t$ errores, la distancia mínima del código debe ser $d \\geq t+1$, lo que implica que no hay combinaciones lineales de $t$ columnas de $H$ que den $\\mathbf{0}$."
            },
            {
              "id": "AL_BAS_013",
              "subCategory": "Espacios Duales",
              "text": "Si $V$ es un espacio vectorial de dimensión finita, ¿qué dimensión tiene su espacio dual $V^*$?",
              "hint": "Usa la base dual asociada a una base de $V$.",
              "options": [
                "$\\dim(V) - 1$",
                "$\\dim(V)$",
                "$2 \\dim(V)$",
                "$\\dim(V)^2$"
              ],
              "correctAnswer": "$\\dim(V)$",
              "explanation": "El espacio dual $V^*$ (formado por funcionales lineales) tiene la misma dimensión que ($V$). Si ($\\{ \\mathbf{e}_i \\}$) es base de ($V$), ($\\{ \\mathbf{e}^*_i \\}$) (donde ($\\mathbf{e}^*_i(\\mathbf{e}_j) = \\delta_{ij}$)) es base de $V^*$."
            },
            {
              "id": "AL_BAS_014",
              "subCategory": "Formas Canónicas",
              "text": "¿Qué tipo de matriz es la forma canónica de Jordan de una matriz $A$?",
              "hint": "Combina bloques diagonales y elementos no nulos en la superdiagonal.",
              "options": [
                "Diagonal",
                "Triangular superior con unos en la superdiagonal",
                "Simétrica",
                "Ortogonal"
              ],
              "correctAnswer": "Triangular superior con unos en la superdiagonal",
              "explanation": "La forma de Jordan es una matriz triangular superior por bloques, donde cada bloque (bloque de Jordan) tiene un valor propio en la diagonal y unos en la superdiagonal."
            },
            {
              "id": "AL_BAS_015",
              "subCategory": "Descomposición SVD",
              "text": "En la descomposición SVD $A = U \\Sigma V^T$, ¿qué propiedad tienen $U$ y $V$?",
              "hint": "Son matrices derivadas de los vectores singulares.",
              "options": [
                "Ambas son triangulares inferiores",
                "$U$ es ortogonal y $V$ es diagonal",
                "$U$ y $V$ son matrices ortogonales",
                "$U$ y $V$ son simétricas"
              ],
              "correctAnswer": "$U$ y $V$ son matrices ortogonales",
              "explanation": "En la SVD, $U$ (vectores singulares izquierdos) y $V$ (vectores singulares derechos) son matrices ortogonales, y $\\Sigma$ es diagonal con valores singulares no negativos."
            }
        ],
        "intermedio": [
            {
              "id": "AL3_INT_001",
              "subCategory": "Proceso de Gram-Schmidt",
              "text": "¿Cuál es el objetivo fundamental del proceso de Gram-Schmidt en un espacio vectorial con producto interno?",
              "hint": "Considera las propiedades de ortogonalidad y normalización.",
              "options": [
                "Transformar un conjunto linealmente independiente en un conjunto ortonormal que genere el mismo subespacio",
                "Calcular el determinante de una matriz asociada",
                "Encontrar los valores propios de una transformación lineal",
                "Resolver sistemas de ecuaciones lineales inconsistentes"
              ],
              "correctAnswer": "Transformar un conjunto linealmente independiente en un conjunto ortonormal que genere el mismo subespacio",
              "explanation": "El proceso de Gram-Schmidt construye recursivamente una base ortonormal a partir de una base cualquiera, preservando el subespacio generado. Si $\\{v_1,...,v_n\\}$ es base, el algoritmo calcula $u_k = v_k - \\sum_{i=1}^{k-1}\\text{proy}_{u_i}(v_k)$ y normaliza."
            },
            {
              "id": "AL3_INT_002",
              "subCategory": "Proceso de Gram-Schmidt",
              "text": "En el paso $k$-ésimo de Gram-Schmidt, si $\\{u_1,...,u_{k-1}\\}$ es ortonormal, ¿cómo se obtiene $u_k$ a partir de $v_k$?",
              "hint": "Usa la fórmula de proyección ortogonal sobre el subespacio generado.",
              "options": [
                "$u_k = v_k - \\sum_{i=1}^{k-1}\\langle v_k, u_i \\rangle u_i$ seguido de normalización",
                "$u_k = v_k + \\sum_{i=1}^{k-1}\\langle v_k, u_i \\rangle u_i$",
                "$u_k = \\frac{v_k}{\\|v_k\\|}$ sin considerar los $u_i$ anteriores",
                "$u_k = v_k \\times u_{k-1}$ (producto cruz)"
              ],
              "correctAnswer": "$u_k = v_k - \\sum_{i=1}^{k-1}\\langle v_k, u_i \\rangle u_i$ seguido de normalización",
              "explanation": "El vector $u_k$ se obtiene restando a $v_k$ sus proyecciones sobre cada $u_i$ ya calculado (para garantizar ortogonalidad), luego se normaliza: $u_k = \\frac{u_k}{\\|u_k\\|}$. Esto asegura que $\\langle u_k, u_i \\rangle = 0$ para $i < k$."
            },
            {
              "id": "AL3_INT_003",
              "subCategory": "Complemento Ortogonal",
              "text": "Dado un subespacio $W \\subseteq V$ con producto interno, ¿cuál es la definición precisa de $W^\\perp$?",
              "hint": "Considera la ortogonalidad con todos los elementos del subespacio.",
              "options": [
                "$W^\\perp = \\{v \\in V \\mid \\langle v, w \\rangle = 0 \\ \\forall w \\in W\\}$",
                "$W^\\perp = \\{v \\in W \\mid \\langle v, v \\rangle = 1\\}$",
                "$W^\\perp = \\{v + w \\mid v \\in V, w \\in W\\}$",
                "$W^\\perp = W$"
              ],
              "correctAnswer": "$W^\\perp = \\{v \\in V \\mid \\langle v, w \\rangle = 0 \\ \\forall w \\in W\\}$",
              "explanation": "El complemento ortogonal $W^\\perp$ contiene todos los vectores en $V$ que son ortogonales a cada vector de $W$. Si $\\dim(V) = n$ y $\\dim(W) = k$, entonces $\\dim(W^\\perp) = n-k$."
            },
            {
              "id": "AL3_INT_004",
              "subCategory": "Proyección Ortogonal",
              "text": "Para un subespacio $W$ con base ortonormal $\\{u_1,...,u_k\\}$, ¿por qué la proyección de $x$ sobre $W$ es $\\sum_{i=1}^k \\langle x, u_i \\rangle u_i$?",
              "hint": "Usa la propiedad de mínima distancia.",
              "options": [
                "Minimiza $\\|x - \\text{proy}_W(x)\\|$ y usa ortogonalidad de la base",
                "Es la media aritmética de los vectores de $W$",
                "Porque $x$ es combinación lineal de los $u_i$",
                "Es un resultado válido solo para $W = V$"
              ],
              "correctAnswer": "Minimiza $\\|x - \\text{proy}_W(x)\\|$ y usa ortogonalidad de la base",
              "explanation": "Esta fórmula surge porque: 1) La proyección es el vector en $W$ más cercano a $x$ (mínima distancia), y 2) Los coeficientes $\\langle x, u_i \\rangle$ son las coordenadas en la base ortonormal (por Parseval)."
            },
            {
              "id": "AL3_INT_005",
              "subCategory": "Descomposición Ortogonal",
              "text": "En la descomposición $V = W \\oplus W^\\perp$, ¿qué teorema garantiza que todo $v \\in V$ puede escribirse como $v = w + w^\\perp$ con $w \\in W$ y $w^\\perp \\in W^\\perp$?",
              "hint": "Es un teorema fundamental en espacios con producto interno.",
              "options": [
                "Teorema de la Proyección Ortogonal",
                "Teorema Fundamental del Álgebra",
                "Teorema de Cayley-Hamilton",
                "Teorema de Rouché-Frobenius"
              ],
              "correctAnswer": "Teorema de la Proyección Ortogonal",
              "explanation": "Este teorema establece que para cualquier subespacio $W$ de un espacio con producto interno $V$, existe una descomposición única $v = w + w^\\perp$, donde $w$ es la proyección ortogonal de $v$ sobre $W$."
            },
            {
              "id": "AL3_INT_006",
              "subCategory": "Matriz de Proyección",
              "text": "Si $P$ es la matriz de proyección sobre un subespacio $W$, ¿por qué cumple $P^2 = P$?",
              "hint": "Piensa en qué ocurre al aplicar la proyección dos veces.",
              "options": [
                "Porque proyectar un vector ya proyectado no cambia el resultado",
                "Porque $P$ es invertible",
                "Porque $P$ es antisimétrica",
                "Porque $P$ tiene determinante 1"
              ],
              "correctAnswer": "Porque proyectar un vector ya proyectado no cambia el resultado",
              "explanation": "La idempotencia ($P^2 = P$) refleja que una proyección es una transformación lineal que, al aplicarse nuevamente a su resultado, no produce cambios. Si $w \\in W$, entonces $Pw = w$ y $P(Pw) = Pw$."
            },
            {
              "id": "AL3_INT_007",
              "subCategory": "Factorización QR",
              "text": "Dada una matriz $A$ de rango completo con columnas $\\{a_1,...,a_n\\}$, ¿cómo se relaciona la factorización $A = QR$ con Gram-Schmidt?",
              "hint": "Las columnas de $Q$ son vectores ortonormalizados.",
              "options": [
                "$Q$ contiene la base ortonormal resultante de aplicar Gram-Schmidt a las columnas de $A$, y $R$ registra los coeficientes de las combinaciones lineales",
                "$R$ es la matriz identidad",
                "$Q$ es diagonal y $R$ triangular inferior",
                "No hay relación directa"
              ],
              "correctAnswer": "$Q$ contiene la base ortonormal resultante de aplicar Gram-Schmidt a las columnas de $A$, y $R$ registra los coeficientes de las combinaciones lineales",
              "explanation": "En $A = QR$: 1) Las columnas de $Q$ son los vectores $u_i$ obtenidos al ortonormalizar las columnas de $A$ via Gram-Schmidt, y 2) $R$ es triangular superior con $R_{ij} = \\langle a_i, u_j \\rangle$ (para $i \\leq j$)."
            },
            {
              "id": "AL3_INT_008",
              "subCategory": "Factorización QR",
              "text": "¿Por qué es útil la factorización QR para resolver sistemas $Ax = b$ numéricamente?",
              "hint": "Considera la estabilidad numérica y la estructura triangular.",
              "options": [
                "Transforma el sistema en uno triangular superior ($Rx = Q^Tb$) que es estable y fácil de resolver",
                "Porque $Q$ siempre es la matriz identidad",
                "Porque permite calcular $A^{-1}$ directamente",
                "Solo es útil para matrices singulares"
              ],
              "correctAnswer": "Transforma el sistema en uno triangular superior ($Rx = Q^Tb$) que es estable y fácil de resolver",
              "explanation": "Al escribir $A = QR$, el sistema se convierte en $QRx = b$. Multiplicando por $Q^T$ (que es estable por ser ortogonal) obtenemos $Rx = Q^Tb$, donde $R$ es triangular superior. Esto evita problemas numéricos al calcular $(A^TA)^{-1}$ en mínimos cuadrados."
            },
            {
              "id": "AL3_INT_009",
              "subCategory": "Códigos Lineales",
              "text": "Para un código lineal $C$ con matriz generadora $G$ y matriz de control $H$, ¿qué condición debe cumplir $H$?",
              "hint": "Piensa en el espacio nulo de $H$.",
              "options": [
                "$H G^T = 0$ (matriz cero)",
                "$H = G^T$",
                "$H$ debe ser invertible",
                "$G H^T = I$ (matriz identidad)"
              ],
              "correctAnswer": "$H G^T = 0$ (matriz cero)",
              "explanation": "Las filas de $H$ generan el espacio nulo de $C$, por lo que para cualquier palabra código $c = xG$ (con $x$ vector mensaje), se cumple $H c^T = H G^T x^T = 0$. Esto permite detectar errores como síndromes no nulos."
            },
            {
              "id": "AL3_INT_010",
              "subCategory": "Códigos Lineales",
              "text": "En un código $[n,k,d]$, ¿qué representa $d$ y cómo se relaciona con la matriz $H$?",
              "hint": "Considera la dependencia lineal de columnas de $H$.",
              "options": [
                "$d$ es la distancia mínima del código y es igual al mínimo número de columnas linealmente dependientes de $H$",
                "$d$ es la dimensión de $H$",
                "$d$ es el número de filas de $G$",
                "$d = n - k + 1$ siempre"
              ],
              "correctAnswer": "$d$ es la distancia mínima del código y es igual al mínimo número de columnas linealmente dependientes de $H$",
              "explanation": "La distancia mínima $d$ es el peso mínimo de una palabra código no nula. Equivalentemente, es el menor número de columnas de $H$ que son linealmente dependientes (pues $c$ es palabra código sí y solo si $Hc^T = 0$)."
            },
            {
              "id": "AL3_INT_011",
              "subCategory": "Distancia Mínima",
              "text": "¿Por qué un código con distancia mínima $d$ puede corregir hasta $\\lfloor \\frac{d-1}{2} \\rfloor$ errores?",
              "hint": "Considera esferas de radio $t$ alrededor de las palabras código.",
              "options": [
                "Porque las esferas de radio $t$ alrededor de las palabras código son disjuntas si $2t + 1 \\leq d$",
                "Porque $d$ es siempre par",
                "Porque el número de errores corregibles es $d^2$",
                "Solo si el código es perfecto"
              ],
              "correctAnswer": "Porque las esferas de radio $t$ alrededor de las palabras código son disjuntas si $2t + 1 \\leq d$",
              "explanation": "La condición $2t + 1 \\leq d$ asegura que las esferas de radio $t$ (vectores a distancia $\\leq t$ de una palabra código) no se solapen. Así, cualquier vector recibido con $\\leq t$ errores estará más cerca de una única palabra código."
            },
            {
              "id": "AL3_INT_012",
              "subCategory": "Mínimos Cuadrados",
              "text": "¿Por qué la solución de mínimos cuadrados de $Ax \\approx b$ está dada por $x = (A^TA)^{-1}A^T b$?",
              "hint": "Deriva las ecuaciones normales proyectando $b$ en $\\text{Col}(A)$.",
              "options": [
                "Porque minimiza $\\|Ax - b\\|^2$ y resuelve $A^TAx = A^Tb$ (ecuaciones normales)",
                "Porque $A$ es siempre invertible",
                "Porque $Ax = b$ tiene solución exacta",
                "Es válido solo si $b \\in \\text{Col}(A)$"
              ],
              "correctAnswer": "Porque minimiza $\\|Ax - b\\|^2$ y resuelve $A^TAx = A^Tb$ (ecuaciones normales)",
              "explanation": "Geométricamente, minimizar $\\|Ax - b\\|^2$ equivale a proyectar $b$ ortogonalmente sobre $\\text{Col}(A)$. La proyección satisface $A^T(b - Ax) = 0$, lo que lleva a las ecuaciones normales $A^TAx = A^Tb$."
            },
            {
              "id": "AL3_INT_013",
              "subCategory": "Pseudoinversa",
              "text": "Si $A = U\\Sigma V^T$ es la SVD de $A$, ¿cómo se expresa la pseudoinversa $A^+$?",
              "hint": "Invierte los valores singulares no nulos.",
              "options": [
                "$A^+ = V\\Sigma^+ U^T$ donde $\\Sigma^+$ reemplaza $\\sigma_i$ por $1/\\sigma_i$ si $\\sigma_i \\neq 0$",
                "$A^+ = A^T$",
                "$A^+ = \\Sigma^{-1}$",
                "$A^+ = U^TV$"
              ],
              "correctAnswer": "$A^+ = V\\Sigma^+ U^T$ donde $\\Sigma^+$ reemplaza $\\sigma_i$ por $1/\\sigma_i$ si $\\sigma_i \\neq 0$",
              "explanation": "La pseudoinversa generaliza la inversa para matrices no cuadradas o singulares. En la SVD, se construye invirtiendo los valores singulares no nulos: si $\\Sigma = \\text{diag}(\\sigma_1,...,\\sigma_r,0,...,0)$, entonces $\\Sigma^+ = \\text{diag}(1/\\sigma_1,...,1/\\sigma_r,0,...,0)$."
            },
            {
              "id": "AL3_INT_014",
              "subCategory": "Aplicaciones SVD",
              "text": "¿Cómo se usa la SVD para resolver un sistema $Ax = b$ en el sentido de mínimos cuadrados?",
              "hint": "Usa la pseudoinversa y la descomposición en valores singulares.",
              "options": [
                "La solución es $x = V\\Sigma^+ U^T b$ donde $\\Sigma^+$ es la pseudoinversa de $\\Sigma$",
                "Calculando $A^{-1}b$ directamente",
                "Resolviendo $\\Sigma x = U^T b V$",
                "Solo es posible si $A$ es cuadrada"
              ],
              "correctAnswer": "La solución es $x = V\\Sigma^+ U^T b$ donde $\\Sigma^+$ es la pseudoinversa de $\\Sigma$",
              "explanation": "La solución de mínimos cuadrados de norma mínima se obtiene aplicando la pseudoinversa: $x = A^+b = V\\Sigma^+ U^T b$. Esto proyecta $b$ sobre $\\text{Col}(A)$ y resuelve el sistema en el subespacio relevante."
            },
            {
              "id": "AL3_INT_015",
              "subCategory": "Mínimos Cuadrados",
              "text": "¿Qué ventaja numérica tiene usar SVD sobre las ecuaciones normales $(A^TA)x = A^Tb$ para mínimos cuadrados?",
              "hint": "Considera el número de condición de las matrices.",
              "options": [
                "La SVD es más estable numéricamente, especialmente cuando $A^TA$ está mal condicionada",
                "Las ecuaciones normales siempre son más precisas",
                "La SVD requiere menos operaciones computacionales",
                "No hay diferencia práctica"
              ],
              "correctAnswer": "La SVD es más estable numéricamente, especialmente cuando $A^TA$ está mal condicionada",
              "explanation": "Calcular $(A^TA)^{-1}$ puede amplificar errores numéricos si $A^TA$ tiene un número de condición alto (casi singular). La SVD evita este problema al trabajar directamente con los valores singulares de $A$, proporcionando mayor estabilidad."
            }
        ],
        "avanzado": [
            {
              "id": "AL3_ADV_001",
              "subCategory": "Descomposición en Valores Singulares",
              "text": "En la SVD reducida $A = U_r Sigma_r V_r^T$ de una matriz $A in mathbb{R}^{m \times n}$ con rango $r$, ¿qué propiedad geométrica caracteriza a las columnas de $U_r$?",
              "hint": "Considera la transformación de la esfera unitaria bajo $A$.",
              "options": [
                "Forman una base ortonormal para el espacio columna de $A$",
                "Son los vectores propios de $A$ con mayor valor propio",
                "Coinciden con las filas de $A$ normalizadas",
                "Generan el espacio nulo de $A^T$"
              ],
              "correctAnswer": "Forman una base ortonormal para el espacio columna de $A$",
              "explanation": "Las columnas de $U_r$ (vectores singulares izquierdos) son una base ortonormal para $\\text{Col}(A)$. Geométricamente, estos vectores definen los semiejes de la elipse que resulta de aplicar $A$ a la esfera unitaria en $mathbb{R}^n$."
            },
            {
              "id": "AL3_ADV_002",
              "subCategory": "Descomposición en Valores Singulares",
              "text": "Si $sigma_i$ son los valores singulares de $A in mathbb{R}^{m \times n}$, ¿cómo se expresa la norma de Frobenius $\\|A\\|_F$ en términos de los $sigma_i$?",
              "hint": "Usa la invariancia de la norma bajo transformaciones unitarias.",
              "options": [
                "$\\sqrt{sum_{i=1}^r sigma_i^2}$ donde $r = \text{rango}(A)$",
                "$\\sum_{i=1}^r sigma_i$",
                "$\\max_i sigma_i$",
                "$\\prod_{i=1}^r sigma_i$"
              ],
              "correctAnswer": "$\\sqrt{sum_{i=1}^r sigma_i^2}$ donde $r = \text{rango}(A)$",
              "explanation": "La norma de Frobenius satisface $\\|A\\|_F^2 = sum_{i,j} |a_{ij}|^2 = sum_{i=1}^r sigma_i^2$ (por invariancia bajo $U$ y $V$). Esto generaliza el teorema de Pitágoras para matrices."
            },
            {
              "id": "AL3_ADV_003",
              "subCategory": "Rango y SVD",
              "text": "Para una matriz $A$ con SVD $A = USigma V^T$ y $epsilon > 0$, ¿cómo se define numéricamente su rango efectivo?",
              "hint": "Considera valores singulares significativos frente a ruido numérico.",
              "options": [
                "Número de $sigma_i > epsilon sigma_1$",
                "Número total de valores singulares no nulos",
                "Dimensión del espacio nulo de $A$",
                "Rango de $A + epsilon I$"
              ],
              "correctAnswer": "Número de $sigma_i > epsilon sigma_1$",
              "explanation": "En aplicaciones numéricas, el rango efectivo cuenta los valores singulares significativos (típicamente mayores que $epsilon sigma_1$, donde $sigma_1$ es el mayor valor singular). Esto evita considerar valores pequeños por ruido numérico como relevantes."
            },
            {
              "id": "AL3_ADV_004",
              "subCategory": "Aproximación de Rango Bajo",
              "text": "Según el teorema de Eckart-Young, ¿por qué $A_k = sum_{i=1}^k sigma_i u_i v_i^T$ es la mejor aproximación de rango $k$ a $A$?",
              "hint": "Considera la minimización de la norma espectral.",
              "options": [
                "Minimiza tanto $\\|A - A_k\\|_2$ como $\\|A - A_k\\|_F$ entre todas las matrices de rango $leq k$",
                "Es la única matriz que coincide con $A$ en las primeras $k$ columnas",
                "Maximiza la traza de $A^T A_k$",
                "Minimiza el número de condición de $A_k$"
              ],
              "correctAnswer": "Minimiza tanto $\\|A - A_k\\|_2$ como $\\|A - A_k\\|_F$ entre todas las matrices de rango $leq k$",
              "explanation": "El teorema de Eckart-Young establece que $A_k$ (truncamiento SVD a $k$ términos) es la solución óptima para ambas normas, con errores $\\|A - A_k\\|_2 = sigma_{k+1}$ y $\\|A - A_k\\|_F = sqrt{sum_{i=k+1}^r sigma_i^2}$."
            },
            {
              "id": "AL3_ADV_005",
              "subCategory": "Análisis de Componentes Principales",
              "text": "En PCA, si $X$ es la matriz de datos centrados, ¿por qué se diagonaliza $X^T X/(n-1)$?",
              "hint": "Relaciona con la matriz de covarianza muestral.",
              "options": [
                "Porque sus autovectores definen las direcciones de máxima varianza en los datos",
                "Porque es equivalente a calcular la media de $X$",
                "Porque diagonaliza directamente la matriz $X$",
                "Porque minimiza la traza de $X$"
              ],
              "correctAnswer": "Porque sus autovectores definen las direcciones de máxima varianza en los datos",
              "explanation": "La matriz $X^T X/(n-1)$ es la matriz de covarianza muestral. Sus autovectores (componentes principales) maximizan $v^T Sigma v$ (varianza proyectada), y los autovalores corresponden a las varianzas en esas direcciones."
            },
            {
              "id": "AL3_ADV_006",
              "subCategory": "Códigos de Reed-Solomon",
              "text": "¿Por qué los códigos Reed-Solomon pueden corregir hasta $t$ errores en $n$ símbolos?",
              "hint": "Considera el grado del polinomio generador y el teorema fundamental del álgebra.",
              "options": [
                "Porque requieren $2t$ símbolos de redundancia y el polinomio de síndrome tiene $2t$ raíces",
                "Porque operan sobre campos finitos primos",
                "Porque usan transformadas de Fourier discretas",
                "Porque limitan el número de símbolos a $n leq q$"
              ],
              "correctAnswer": "Porque requieren $2t$ símbolos de redundancia y el polinomio de síndrome tiene $2t$ raíces",
              "explanation": "Un código RS$(n,k)$ con $n-k=2t$ puede corregir hasta $t$ errores. Esto se debe a que el locador de errores es un polinomio de grado $t$ determinado por $2t$ síndromes (ecuaciones de Newton)."
            },
            {
              "id": "AL3_ADV_007",
              "subCategory": "Códigos BCH",
              "text": "¿Cómo aprovechan los códigos BCH las propiedades de los campos finitos para corregir errores?",
              "hint": "Considera los polinomios mínimos y las raíces primitivas.",
              "options": [
                "Usan raíces consecutivas $alpha^b, alpha^{b+1},..., alpha^{b+2t-1}$ para construir un polinomio generador con distancia $d geq 2t+1$",
                "Codifican directamente en la base estándar de $mathbb{F}_q$",
                "Maximizan el determinante de la matriz de control",
                "Minimizan el número de raíces del polinomio generador"
              ],
              "correctAnswer": "Usan raíces consecutivas $alpha^b, alpha^{b+1},..., alpha^{b+2t-1}$ para construir un polinomio generador con distancia $d geq 2t+1$",
              "explanation": "Los códigos BCH especifican $2t$ raíces consecutivas del polinomio generador $g(x)$, garantizando (vía el bound BCH) que el peso mínimo del código es $d geq 2t+1$, permitiendo corregir $t$ errores."
            },
            {
              "id": "AL3_ADV_008",
              "subCategory": "Factorización QR con Pivoteo",
              "text": "En QR con pivoteo de columnas, ¿por qué se permutan las columnas de $A$ durante la factorización?",
              "hint": "Considera el crecimiento de errores numéricos en columnas casi dependientes.",
              "options": [
                "Para priorizar columnas con mayor norma residual y mejorar estabilidad numérica",
                "Para hacer $R$ diagonal",
                "Para garantizar que $Q$ sea la matriz identidad",
                "Para reducir el número de operaciones"
              ],
              "correctAnswer": "Para priorizar columnas con mayor norma residual y mejorar estabilidad numérica",
              "explanation": "El pivoteo elige en cada paso la columna con mayor norma residual (no aún procesada) como pivote, reduciendo el error numérico al evitar divisiones por elementos pequeños y mejorando la detección del rango numérico."
            },
            {
              "id": "AL3_ADV_009",
              "subCategory": "Regularización Tikhonov",
              "text": "¿Cómo afecta el parámetro $lambda$ en la solución de $\\min \\|Ax-b\\|^2 + lambda \\|x\\|^2$?",
              "hint": "Considera el trade-off entre sesgo y varianza.",
              "options": [
                "Controla el compromiso entre ajuste a los datos ($\\|Ax-b\\|$) y magnitud de la solución ($\\|x\\|$)",
                "Minimiza exclusivamente $\\|x\\|$ sin considerar $b$",
                "Hace que $A$ sea ortogonal",
                "Elimina automáticamente las columnas dependientes de $A$"
              ],
              "correctAnswer": "Controla el compromiso entre ajuste a los datos ($\\|Ax-b\\|$) y magnitud de la solución ($\\|x\\|$)",
              "explanation": "$lambda$ regula el balance: $lambda \to 0$ da la solución clásica (posiblemente inestable), mientras que $lambda > 0$ reduce la varianza a costa de introducir sesgo. Óptimo según el criterio L-curve o validación cruzada."
            },
            {
              "id": "AL3_ADV_010",
              "subCategory": "Mínimos Cuadrados Regularizados",
              "text": "¿Por qué la solución $(A^T A + lambda I)^{-1}A^T b$ evita problemas cuando $A^T A$ es casi singular?",
              "hint": "Analiza los valores propios de la matriz regularizada.",
              "options": [
                "$lambda I$ desplaza los valores propios de $A^T A$ lejos de cero, mejorando el número de condición",
                "Convierte $A$ en una matriz ortogonal",
                "Anula las columnas linealmente dependientes de $A$",
                "Hace que $A^T A$ sea diagonal"
              ],
              "correctAnswer": "$lambda I$ desplaza los valores propios de $A^T A$ lejos de cero, mejorando el número de condición",
              "explanation": "Si $A^T A$ tiene valores propios pequeños $mu_i$, entonces $A^T A + lambda I$ tiene valores propios $mu_i + lambda$, reduciendo el número de condición de $kappa = \frac{mu_{max}}{mu_{min}}$ a $\frac{mu_{max} + lambda}{mu_{min} + lambda}$."
            },
            {
              "id": "AL3_ADV_011",
              "subCategory": "Descomposición Espectral",
              "text": "Para una matriz simétrica $A$, ¿cómo se relaciona su SVD con su descomposición espectral $A = QLambda Q^T$?",
              "hint": "Compara valores singulares con valores propios.",
              "options": [
                "Coinciden: $Sigma = |Lambda|$ y $U = V = Q$ (salvo signos)",
                "La SVD no existe para matrices simétricas",
                "Los valores singulares son los cuadrados de los valores propios",
                "Requieren bases diferentes"
              ],
              "correctAnswer": "Coinciden: $Sigma = |Lambda|$ y $U = V = Q$ (salvo signos)",
              "explanation": "Para $A$ simétrica, los valores singulares son los módulos de los valores propios ($sigma_i = |lambda_i|$). Si $A$ es además definida positiva, SVD y descomposición espectral son idénticas con $U = V = Q$."
            },
            {
              "id": "AL3_ADV_012",
              "subCategory": "Normas de Matrices",
              "text": "¿Por qué la norma espectral $\\|A\\|_2$ coincide con el mayor valor singular $sigma_1$?",
              "hint": "Usa la interpretación geométrica de la transformación lineal.",
              "options": [
                "Porque maximiza el cociente $\\frac{\\|Ax\\|}{\\|x\\|}$ sobre todos los $x \neq 0$",
                "Porque es igual a la traza de $A^T A$",
                "Porque siempre coincide con el radio espectral $\rho(A)$",
                "Porque minimiza el error de aproximación de rango bajo"
              ],
              "correctAnswer": "Porque maximiza el cociente $\\frac{\\|Ax\\|}{\\|x\\|}$ sobre todos los $x \neq 0$",
              "explanation": "La norma espectral es el máximo factor de estiramiento: $\\|A\\|_2 = sup_{x \neq 0} \\frac{\\|Ax\\|_2}{\\|x\\|_2} = sigma_1$, donde $sigma_1$ es el semieje mayor de la elipse imagen de la esfera unitaria bajo $A$."
            },
            {
              "id": "AL3_ADV_013",
              "subCategory": "Condicionamiento",
              "text": "Si $kappa_2(A) = 10^{16}$, ¿qué implica esto numéricamente para el sistema $Ax = b$?",
              "hint": "Considera la pérdida de dígitos significativos.",
              "options": [
                "Que el sistema es esencialmente singular: se pueden perder hasta 16 dígitos de precisión",
                "Que $A$ es ortogonal",
                "Que la solución es insensible a perturbaciones",
                "Que $A$ es diagonal dominante"
              ],
              "correctAnswer": "Que el sistema es esencialmente singular: se pueden perder hasta 16 dígitos de precisión",
              "explanation": "Un número de condición $kappa_2(A) approx 10^{k}$ indica que, en aritmética de precisión $p$, solo $p-k$ dígitos son confiables en la solución. Para $kappa=10^{16}$ y doble precisión ($p=16$), la solución puede ser numéricamente inexacta."
            },
            {
              "id": "AL3_ADV_014",
              "subCategory": "Métodos Iterativos",
              "text": "En GMRES, ¿por qué se usa el subespacio de Krylov $mathcal{K}_k(A,b) = \text{span}{b, Ab, ..., A^{k-1}b}$?",
              "hint": "Considera la minimización del residuo en espacios anidados.",
              "options": [
                "Porque captura progresivamente las direcciones de mayor impacto en el residuo $b - Ax$",
                "Porque diagonaliza $A$ en cada iteración",
                "Porque garantiza convergencia en exactamente $n$ pasos",
                "Porque evita el cálculo de productos matriz-vector"
              ],
              "correctAnswer": "Porque captura progresivamente las direcciones de mayor impacto en el residuo $b - Ax$",
              "explanation": "GMRES construye una base ortonormal para $mathcal{K}_k(A,b)$ donde minimiza $\\|b - Ax_k\\|_2$. El subespacio de Krylov contiene información sobre las direcciones donde $A$ más influye en $b$, acelerando la convergencia."
            },
            {
              "id": "AL3_ADV_015",
              "subCategory": "Optimización Convexa",
              "text": "Para $f$ convexo y $L$-suave, ¿qué garantiza el paso de descenso de gradiente $x_{k+1} = x_k - eta \nabla f(x_k)$ con $eta = 1/L$?",
              "hint": "Considera la condición de Lipschitz del gradiente.",
              "options": [
                "Convergencia con tasa $O(1/k)$ en el valor objetivo",
                "Optimalidad en exactamente $k = n$ pasos",
                "Minimización exacta en cada iteración",
                "Convergencia superlineal"
              ],
              "correctAnswer": "Convergencia con tasa $O(1/k)$ en el valor objetivo",
              "explanation": "Para funciones $L$-suaves (gradiente Lipschitz), el descenso de gradiente con $eta = 1/L$ garantiza $f(x_k) - f(x^*) leq \frac{L\\|x_0 - x^*\\|^2}{2k}$. La condición $L$ controla el tamaño de paso máximo para evitar oscilaciones."
            }
        ]
      }
    },
    {
      "id": "algebra_lineal_4",
      "name": "Álgebra Lineal IV",
      "description": "Preguntas sobre autovalores y autovectores, ecuaciones en diferencias, y formas cuadráticas.",
      "difficultyLevels": {
        "basico": [
            {
              "id": "AL_EASY_001",
              "subCategory": "Autovalores y Autovectores",
              "text": "¿Qué es un autovalor de una matriz A?",
              "hint": "Piensa en cómo ciertos vectores mantienen su dirección bajo la transformación lineal.",
              "options": [
                "Un escalar λ tal que Av = λv para algún vector v ≠ 0",
                "Un vector v tal que Av = λv para algún escalar λ",
                "La diagonal principal de la matriz A",
                "El determinante de la matriz A"
              ],
              "correctAnswer": "Un escalar λ tal que Av = λv para algún vector v ≠ 0",
              "explanation": "Un autovalor (λ) es un escalar asociado a una transformación lineal (matriz A) que representa el factor de escala por el cual un autovector (v) es estirado o contraído. Formalmente, satisface Av = λv, donde v ≠ 0. Geométricamente, los autovectores definen las direcciones invariantes bajo la aplicación de A."
            },
            {
              "id": "AL_EASY_002",
              "subCategory": "Autovalores y Autovectores",
              "text": "¿Cómo se llama al vector v en la ecuación Av = λv?",
              "hint": "Este vector no cambia de dirección bajo la transformación, solo de magnitud.",
              "options": [
                "Autovector",
                "Vector propio",
                "Eigenvector",
                "Todas las anteriores"
              ],
              "correctAnswer": "Todas las anteriores",
              "explanation": "El término 'autovector' (del griego 'auto' = propio), 'vector propio' (traducción directa) y 'eigenvector' (del alemán 'eigen' = propio) son equivalentes. Todos se refieren al vector v que satisface Av = λv y que, por tanto, solo es escalado por la transformación lineal."
            },
            {
              "id": "AL_EASY_003",
              "subCategory": "Polinomio Característico",
              "text": "¿Cómo se define el polinomio característico de una matriz A?",
              "hint": "Está relacionado con la búsqueda de escalares que hacen a A - λI singular.",
              "options": [
                "det(A - λI)",
                "det(A + λI)",
                "det(λI - A)",
                "det(A)"
              ],
              "correctAnswer": "det(A - λI)",
              "explanation": "El polinomio característico se define como p(λ) = det(A - λI). Sus raíces son los autovalores de A, ya que det(A - λI) = 0 garantiza la existencia de un núcleo no trivial (autovectores). Nota: det(λI - A) también es válido (difiere en un signo si n es impar)."
            },
            {
              "id": "AL_EASY_004",
              "subCategory": "Autovalores",
              "text": "Los autovalores de una matriz son:",
              "hint": "Son los valores que hacen que la matriz A - λI pierda rango.",
              "options": [
                "Las raíces del polinomio característico",
                "Los elementos de la diagonal principal",
                "Los coeficientes del polinomio característico",
                "Los vectores columna de la matriz"
              ],
              "correctAnswer": "Las raíces del polinomio característico",
              "explanation": "Los autovalores son las raíces de det(A - λI) = 0. Solo para matrices triangulares coinciden con los elementos diagonales. Los coeficientes del polinomio característico están relacionados con trazas y determinantes (e.g., el término constante es det(A))."
            },
            {
              "id": "AL_EASY_005",
              "subCategory": "Matrices Diagonales",
              "text": "¿Cuáles son los autovalores de una matriz diagonal?",
              "hint": "Observa qué sucede con A - λI cuando A es diagonal.",
              "options": [
                "Los elementos de la diagonal principal",
                "La suma de los elementos de la diagonal",
                "El producto de los elementos de la diagonal",
                "Los elementos fuera de la diagonal"
              ],
              "correctAnswer": "Los elementos de la diagonal principal",
              "explanation": "Para una matriz diagonal A = diag(a₁₁, a₂₂, ..., aₙₙ), el polinomio característico es (a₁₁ - λ)(a₂₂ - λ)...(aₙₙ - λ). Por tanto, los autovalores son exactamente los elementos aᵢᵢ. Esto refleja que los autovectores son los vectores canónicos eᵢ."
            },
            {
              "id": "AL_EASY_006",
              "subCategory": "Traza",
              "text": "¿Qué relación existe entre la traza de una matriz y sus autovalores?",
              "hint": "Considera la suma de las raíces del polinomio característico.",
              "options": [
                "La traza es la suma de todos los autovalores",
                "La traza es el producto de todos los autovalores",
                "La traza es el mayor autovalor",
                "No hay relación entre ellos"
              ],
              "correctAnswer": "La traza es la suma de todos los autovalores",
              "explanation": "La traza de A (suma de elementos diagonales) coincide con la suma de autovalores (incluyendo multiplicidades). Esto se deriva de que el coeficiente de λⁿ⁻¹ en p(λ) es (-1)ⁿ⁻¹·tr(A). Ejemplo: Para A = [[2, 1], [0, 3]], tr(A) = 5 y autovalores son 2 y 3."
            },
            {
              "id": "AL_EASY_007",
              "subCategory": "Determinante",
              "text": "¿Qué relación existe entre el determinante de una matriz y sus autovalores?",
              "hint": "Piensa en cómo el determinante se relaciona con el término constante del polinomio característico.",
              "options": [
                "El determinante es el producto de todos los autovalores",
                "El determinante es la suma de todos los autovalores",
                "El determinante es el mayor autovalor",
                "No hay relación entre ellos"
              ],
              "correctAnswer": "El determinante es el producto de todos los autovalores",
              "explanation": "El determinante de A es igual al producto de sus autovalores (con multiplicidad). Esto surge de evaluar p(λ) en λ = 0: det(A) = p(0) = (-1)ⁿ·λ₁λ₂...λₙ. Además, explica por qué matrices con autovalor cero son singulares."
            },
            {
              "id": "AL_EASY_008",
              "subCategory": "Espacios Propios",
              "text": "¿Qué es el espacio propio asociado a un autovalor λ?",
              "hint": "Incluye todos los vectores que son escalados por λ, más el vector nulo.",
              "options": [
                "El conjunto de todos los autovectores asociados a λ más el vector cero",
                "Solo los autovectores asociados a λ",
                "El conjunto de todos los autovalores",
                "La matriz A menos λI"
              ],
              "correctAnswer": "El conjunto de todos los autovectores asociados a λ más el vector cero",
              "explanation": "El espacio propio E_λ es el núcleo de (A - λI), es decir, {v | (A - λI)v = 0}. Es un subespacio vectorial porque: (1) 0 ∈ E_λ, (2) es cerrado bajo suma y multiplicación escalar. Su dimensión es la multiplicidad geométrica de λ."
            },
            {
              "id": "AL_EASY_009",
              "subCategory": "Ecuaciones en Diferencias",
              "text": "¿Qué es una ecuación en diferencias lineal?",
              "hint": "Modela sistemas discretos donde el estado siguiente depende linealmente del actual.",
              "options": [
                "Una ecuación que relaciona los valores de una secuencia en diferentes índices",
                "Una ecuación diferencial ordinaria",
                "Una ecuación que no tiene solución",
                "Una ecuación con coeficientes constantes únicamente"
              ],
              "correctAnswer": "Una ecuación que relaciona los valores de una secuencia en diferentes índices",
              "explanation": "Una ecuación en diferencias lineal expresa xₙ₊₁ como combinación lineal de términos previos (e.g., xₙ₊₁ = axₙ + bxₙ₋₁). Se resuelve usando autovalores de la matriz asociada al sistema. Contrasta con ecuaciones diferenciales, que son continuas."
            },
            {
              "id": "AL_EASY_010",
              "subCategory": "Ecuaciones en Diferencias",
              "text": "Una ecuación en diferencias de primer orden tiene la forma:",
              "hint": "Solo depende del término inmediatamente anterior.",
              "options": [
                "xₙ₊₁ = axₙ + b",
                "xₙ₊₂ = axₙ₊₁ + bxₙ",
                "x'(t) = ax(t) + b",
                "xₙ = axₙ₋₁ + b"
              ],
              "correctAnswer": "xₙ₊₁ = axₙ + b",
              "explanation": "La forma general es xₙ₊₁ = axₙ + b (primer orden porque solo involucra xₙ). Su solución es xₙ = aⁿx₀ + b(1 - aⁿ)/(1 - a) si a ≠ 1. Para a = 1, es xₙ = x₀ + nb. Usa notación de subíndices para claridad en contextos discretos."
            },
            {
              "id": "AL_EASY_011",
              "subCategory": "Formas Cuadráticas",
              "text": "¿Qué es una forma cuadrática?",
              "hint": "Es una función escalar que generaliza las ecuaciones cuadráticas en múltiples variables.",
              "options": [
                "Una función de la forma Q(x) = xᵀAx donde A es simétrica",
                "Una ecuación de segundo grado",
                "Una matriz cuadrada",
                "Un polinomio de grado cuatro"
              ],
              "correctAnswer": "Una función de la forma Q(x) = xᵀAx donde A es simétrica",
              "explanation": "Una forma cuadrática es Q(x) = xᵀAx, con A simétrica. Representa una combinación de términos cuadráticos (xᵢ²) y cruzados (xᵢxⱼ). En geometría, describe cónicas/cuádricas. Ejemplo: Q(x,y) = 2x² + 4xy + 3y² se escribe con A = [[2, 2], [2, 3]]."
            },
            {
              "id": "AL_EASY_012",
              "subCategory": "Formas Cuadráticas",
              "text": "Una forma cuadrática es definida positiva si:",
              "hint": "Implica que la matriz asociada tiene autovalores estrictamente positivos.",
              "options": [
                "Q(x) > 0 para todo x ≠ 0",
                "Q(x) ≥ 0 para todo x",
                "Q(x) < 0 para todo x ≠ 0",
                "Q(x) = 0 para todo x"
              ],
              "correctAnswer": "Q(x) > 0 para todo x ≠ 0",
              "explanation": "Q(x) es definida positiva si Q(x) > 0 ∀ x ≠ 0. Equivalentemente, todos los autovalores de A son positivos. En optimización, esto garantiza que un punto crítico es un mínimo local. Ejemplo: Q(x,y) = x² + y² es definida positiva."
            },
            {
              "id": "AL_EASY_013",
              "subCategory": "Diagonalización",
              "text": "Una matriz A es diagonalizable si:",
              "hint": "Requiere que exista una base de autovectores para el espacio completo.",
              "options": [
                "Existe P invertible tal que P⁻¹AP es diagonal",
                "A es una matriz diagonal",
                "A tiene determinante no nulo",
                "A es simétrica"
              ],
              "correctAnswer": "Existe P invertible tal que P⁻¹AP es diagonal",
              "explanation": "A es diagonalizable si existe una matriz P invertible (cuyas columnas son autovectores de A) y una matriz diagonal D (con autovalores) tales que A = PDP⁻¹. Esto ocurre si y solo si la suma de las multiplicidades geométricas de los autovalores es igual a n."
            },
            {
              "id": "AL_EASY_014",
              "subCategory": "Multiplicidad",
              "text": "¿Qué es la multiplicidad algebraica de un autovalor?",
              "hint": "Se refiere a la cantidad de veces que aparece como raíz del polinomio característico.",
              "options": [
                "El número de veces que λ es raíz de det(A - λI) = 0",
                "La dimensión de ker(A - λI)",
                "El número de autovectores LI asociados a λ",
                "El valor de λ"
              ],
              "correctAnswer": "El número de veces que λ es raíz de det(A - λI) = 0",
              "explanation": "La multiplicidad algebraica (ma) de λ es su multiplicidad como raíz del polinomio característico. Siempre ma ≥ mg (multiplicidad geométrica). Si ma(λ) = mg(λ) para todo λ, entonces A es diagonalizable."
            },
            {
              "id": "AL_EASY_015",
              "subCategory": "Multiplicidad",
              "text": "¿Qué es la multiplicidad geométrica de un autovalor?",
              "hint": "Está relacionada con la dimensión del subespacio de soluciones de (A - λI)v = 0.",
              "options": [
                "La dimensión de ker(A - λI)",
                "El número de veces que λ es raíz de det(A - λI)",
                "El determinante de A - λI",
                "La traza de A"
              ],
              "correctAnswer": "La dimensión de ker(A - λI)",
              "explanation": "La multiplicidad geométrica (mg) de λ es la dimensión del espacio propio E_λ = ker(A - λI), es decir, el número de autovectores linealmente independientes asociados a λ. Siempre mg ≤ ma. Ejemplo: Para A = [[2, 1], [0, 2]], λ=2 tiene ma=2 pero mg=1."
            }
        ],
        "intermedio": [
            {
              "id": "AL_MEDIUM_001",
              "subCategory": "Diagonalización",
              "text": "¿Cuál es la condición necesaria y suficiente para que una matriz sea diagonalizable?",
              "hint": "Piensa en la relación entre los autovectores disponibles y la dimensión del espacio.",
              "options": [
                "La multiplicidad geométrica debe igualar la multiplicidad algebraica para cada autovalor",
                "Todos los autovalores deben ser distintos",
                "La matriz debe ser simétrica",
                "La matriz debe ser invertible"
              ],
              "correctAnswer": "La multiplicidad geométrica debe igualar la multiplicidad algebraica para cada autovalor",
              "explanation": "Una matriz ( A in mathbb{C}^{n \times n} ) es diagonalizable si y solo si para cada autovalor ( lambda ), la multiplicidad geométrica (dimensión del espacio propio ( ker(A - lambda I) )) coincide con su multiplicidad algebraica (multiplicidad como raíz del polinomio característico). Esto garantiza que existan ( n ) autovectores linealmente independientes para formar la matriz ( P ) en ( A = PDP^{-1} ). Ejemplo: ( A = \begin{pmatrix} 1 & 1 \\ 0 & 2 end{pmatrix} ) es diagonalizable (ma=mg=1 para λ=1 y λ=2), pero ( B = \begin{pmatrix} 2 & 1 \\ 0 & 2 end{pmatrix} ) no lo es (para λ=2, ma=2 pero mg=1)."
            },
            {
              "id": "AL_MEDIUM_002",
              "subCategory": "Matrices Simétricas",
              "text": "¿Qué propiedad especial tienen los autovalores de una matriz simétrica real?",
              "hint": "Considera el teorema espectral para matrices simétricas.",
              "options": [
                "Son todos reales",
                "Son todos complejos",
                "Son todos positivos",
                "Son todos iguales"
              ],
              "correctAnswer": "Son todos reales",
              "explanation": "Por el teorema espectral, si ( A ) es una matriz simétrica real (( A = A^T )), entonces todos sus autovalores son números reales. Esto se demuestra usando que ( lambda langle v, v \rangle = langle Av, v \rangle = langle v, Av \rangle = overline{lambda} langle v, v \rangle ), lo que implica ( lambda = overline{lambda} ). Nota: Esto no implica que sean positivos (e.g., ( A = \begin{pmatrix} 0 & 1 \\ 1 & 0 end{pmatrix} ) tiene autovalores ( pm 1 ))."
            },
            {
              "id": "AL_MEDIUM_003",
              "subCategory": "Matrices Simétricas",
              "text": "¿Qué propiedad tienen los autovectores de una matriz simétrica real?",
              "hint": "Usa la ortogonalidad en el teorema espectral.",
              "options": [
                "Autovectores correspondientes a autovalores distintos son ortogonales",
                "Todos los autovectores son paralelos",
                "Los autovectores son complejos",
                "No existen autovectores para matrices simétricas"
              ],
              "correctAnswer": "Autovectores correspondientes a autovalores distintos son ortogonales",
              "explanation": "Para una matriz simétrica real ( A ), autovectores asociados a autovalores distintos son ortogonales. Si ( Av_1 = lambda_1 v_1 ) y ( Av_2 = lambda_2 v_2 ) con ( lambda_1 \neq lambda_2 ), entonces ( lambda_1 langle v_1, v_2 \rangle = langle Av_1, v_2 \rangle = langle v_1, Av_2 \rangle = lambda_2 langle v_1, v_2 \rangle ), lo que implica ( langle v_1, v_2 \rangle = 0 ). Además, siempre existe una base ortonormal de autovectores."
            },
            {
              "id": "AL_MEDIUM_004",
              "subCategory": "Teorema Espectral",
              "text": "Según el teorema espectral, una matriz simétrica real puede escribirse como:",
              "hint": "La diagonalización usa una matriz ortogonal en este caso.",
              "options": [
                "( A = Q Lambda Q^T ) donde ( Q ) es ortogonal y ( Lambda ) es diagonal",
                "( A = PDP^{-1} ) donde ( P ) y ( D ) son arbitrarias",
                "( A = LU ) donde ( L ) es triangular inferior y ( U ) superior",
                "( A = QR ) donde ( Q ) es ortogonal y ( R ) triangular superior"
              ],
              "correctAnswer": "( A = Q Lambda Q^T ) donde ( Q ) es ortogonal y ( Lambda ) es diagonal",
              "explanation": "El teorema espectral establece que toda matriz simétrica real ( A ) puede diagonalizarse como ( A = Q Lambda Q^T ), donde ( Q ) es una matriz ortogonal (( Q^T = Q^{-1} )) cuyas columnas son autovectores ortonormales de ( A ), y ( Lambda ) es diagonal con los autovalores. Ejemplo: ( A = \begin{pmatrix} 3 & 1 \\ 1 & 3 end{pmatrix} ) se descompone como ( A = Q \begin{pmatrix} 2 & 0 \\ 0 & 4 end{pmatrix} Q^T ) con ( Q = \frac{1}{sqrt{2}} \begin{pmatrix} 1 & -1 \\ 1 & 1 end{pmatrix} )."
            },
            {
              "id": "AL_MEDIUM_005",
              "subCategory": "Ecuaciones en Diferencias",
              "text": "Para la ecuación ( x_{n+1} = a x_n + b ), ¿cuál es la solución general?",
              "hint": "Considera la solución homogénea y una solución particular constante.",
              "options": [
                "( x_n = a^n x_0 + \\frac{b(a^n - 1)}{a - 1} ) si ( a \\neq 1 )",
                "( x_n = a x_0 + n b )",
                "( x_n = a^n + b^n )",
                "( x_n = (a + b)^n )"
              ],
              "correctAnswer": "( x_n = a^n x_0 + \\frac{b(a^n - 1)}{a - 1} ) si ( a \\neq 1 )",
              "explanation": "La solución general se obtiene sumando la solución homogénea (( x_n^{(h)} = C a^n )) y una solución particular constante (( x_n^{(p)} = \\frac{b}{1 - a} )). Para ( a \\neq 1 ), con ( C = x_0 - \\frac{b}{1 - a} ), se llega a ( x_n = a^n x_0 + b \\frac{a^n - 1}{a - 1} ). Si ( a = 1 ), la solución es ( x_n = x_0 + n b )."
            },
            {
              "id": "AL_MEDIUM_006",
              "subCategory": "Ecuaciones en Diferencias",
              "text": "¿Cuándo es estable la solución de ( x_{n+1} = a x_n )?",
              "hint": "La estabilidad depende del comportamiento asintótico cuando ( n \\to infty ).",
              "options": [
                "Cuando ( |a| < 1 )",
                "Cuando ( |a| > 1 )",
                "Cuando ( a > 0 )",
                "Cuando ( a < 0 )"
              ],
              "correctAnswer": "Cuando ( |a| < 1 )",
              "explanation": "La solución ( x_n = a^n x_0 ) es estable (i.e., ( lim_{n \\to infty} x_n = 0 ) para cualquier ( x_0 )) si y solo si ( |a| < 1 ). Si ( |a| geq 1 ), la solución diverge o permanece acotada pero no converge a cero. En sistemas multidimensionales, esto se generaliza al radio espectral de la matriz ( A )."
            },
            {
              "id": "AL_MEDIUM_007",
              "subCategory": "Sistemas de Ecuaciones en Diferencias",
              "text": "Para el sistema ( mathbf{x}_{n+1} = A mathbf{x}_n ), ¿cuál es la solución?",
              "hint": "Generaliza el caso escalar usando potencias matriciales.",
              "options": [
                "( mathbf{x}_n = A^n mathbf{x}_0 )",
                "( mathbf{x}_n = n A mathbf{x}_0 )",
                "( mathbf{x}_n = A mathbf{x}_0 + n )",
                "( mathbf{x}_n = (A + n) mathbf{x}_0 )"
              ],
              "correctAnswer": "( mathbf{x}_n = A^n mathbf{x}_0 )",
              "explanation": "La solución es ( mathbf{x}_n = A^n mathbf{x}_0 ). Si ( A ) es diagonalizable (( A = PDP^{-1} )), entonces ( mathbf{x}_n = P D^n P^{-1} mathbf{x}_0 ), donde ( D^n ) se calcula elevando cada autovalor a la ( n )-ésima potencia. Para ( A ) no diagonalizable, se usa la forma canónica de Jordan."
            },
            {
              "id": "AL_MEDIUM_008",
              "subCategory": "Estabilidad",
              "text": "Un sistema ( mathbf{x}_{n+1} = A mathbf{x}_n ) es estable si:",
              "hint": "La estabilidad depende de los autovalores de ( A ).",
              "options": [
                "Todos los autovalores de ( A ) tienen módulo menor que 1",
                "Todos los autovalores de ( A ) son positivos",
                "Todos los autovalores de ( A ) son reales",
                "La matriz ( A ) es simétrica"
              ],
              "correctAnswer": "Todos los autovalores de ( A ) tienen módulo menor que 1",
              "explanation": "El sistema es estable (todas las soluciones tienden a cero cuando ( n \to infty )) si y solo si el radio espectral ( \rho(A) = max |lambda_i| < 1 ). Esto garantiza que ( |A^n| \to 0 ). Si ( \rho(A) = 1 ), el sistema puede ser marginalmente estable, y si ( \rho(A) > 1 ), es inestable."
            },
            {
              "id": "AL_MEDIUM_009",
              "subCategory": "Formas Cuadráticas",
              "text": "¿Cómo se determina si una forma cuadrática es definida positiva usando autovalores?",
              "hint": "Relaciona la definitud con los signos de los autovalores.",
              "options": [
                "Todos los autovalores de la matriz asociada deben ser positivos",
                "Todos los autovalores deben ser negativos",
                "Al menos un autovalor debe ser positivo",
                "La suma de autovalores debe ser positiva"
              ],
              "correctAnswer": "Todos los autovalores de la matriz asociada deben ser positivos",
              "explanation": "Una forma cuadrática ( Q(mathbf{x}) = mathbf{x}^T A mathbf{x} ) es definida positiva si y solo si todos los autovalores de ( A ) son positivos. Esto implica que ( Q(mathbf{x}) > 0 ) para todo ( mathbf{x} \neq mathbf{0} ). Equivalentemente, los menores principales de ( A ) deben ser positivos (criterio de Sylvester)."
            },
            {
              "id": "AL_MEDIUM_010",
              "subCategory": "Formas Cuadráticas",
              "text": "¿Qué son los menores principales de una matriz?",
              "hint": "Son determinantes de submatrices anidadas.",
              "options": [
                "Los determinantes de las submatrices cuadradas formadas por las primeras ( k ) filas y columnas",
                "Los elementos de la diagonal principal",
                "Los autovalores de la matriz",
                "Los elementos fuera de la diagonal"
              ],
              "correctAnswer": "Los determinantes de las submatrices cuadradas formadas por las primeras ( k ) filas y columnas",
              "explanation": "Los menores principales líderes de ( A ) son ( det(A_k) ) donde ( A_k ) es la submatriz de ( A ) formada por las primeras ( k ) filas y columnas (( k = 1, dots, n )). Por ejemplo, para ( A = \begin{pmatrix} a & b \\ c & d end{pmatrix} ), los menores principales son ( a ) y ( ad - bc ). Son clave en el criterio de Sylvester para definitud positiva."
            },
            {
              "id": "AL_MEDIUM_011",
              "subCategory": "Criterio de Sylvester",
              "text": "Según el criterio de Sylvester, una matriz simétrica es definida positiva si:",
              "hint": "Requiere positividad en todos los menores principales líderes.",
              "options": [
                "Todos sus menores principales líderes son positivos",
                "Todos sus menores principales líderes son negativos",
                "Su determinante es positivo",
                "Su traza es positiva"
              ],
              "correctAnswer": "Todos sus menores principales líderes son positivos",
              "explanation": "El criterio de Sylvester establece que una matriz simétrica ( A ) es definida positiva si y solo si todos sus menores principales líderes son positivos. Esto es más eficiente que calcular autovalores para matrices grandes. Ejemplo: ( A = \begin{pmatrix} 2 & -1 \\ -1 & 2 end{pmatrix} ) es definida positiva porque ( 2 > 0 ) y ( det(A) = 3 > 0 )."
            },
            {
              "id": "AL_MEDIUM_012",
              "subCategory": "Potencias de Matrices",
              "text": "Si ( A = PDP^{-1} ) donde ( D ) es diagonal, entonces ( A^n ) es igual a:",
              "hint": "Usa las propiedades de las potencias matriciales en la diagonalización.",
              "options": [
                "( P D^n P^{-1} )",
                "( (PD)^n (P^{-1})^n )",
                "( P^n D^n (P^{-1})^n )",
                "( PDP^{-1} )"
              ],
              "correctAnswer": "( P D^n P^{-1} )",
              "explanation": "Cuando ( A ) es diagonalizable (( A = PDP^{-1} )), las potencias se calculan como ( A^n = P D^n P^{-1} ), donde ( D^n ) es la matriz diagonal con los autovalores elevados a la ( n )-ésima potencia. Esto simplifica cálculos en ecuaciones en diferencias y sistemas dinámicos."
            },
            {
              "id": "AL_MEDIUM_013",
              "subCategory": "Ecuaciones en Diferencias de Segundo Orden",
              "text": "Para la ecuación ( x_{n+2} + a x_{n+1} + b x_n = 0 ), ¿qué se debe analizar para encontrar la solución?",
              "hint": "La solución depende de las raíces de un polinomio característico asociado.",
              "options": [
                "La ecuación característica ( lambda^2 + a lambda + b = 0 )",
                "Los autovalores de la matriz compañera",
                "El polinomio mínimo",
                "Todas las anteriores"
              ],
              "correctAnswer": "Todas las anteriores",
              "explanation": "Para resolver la recurrencia, se analiza: (1) La ecuación característica ( lambda^2 + a lambda + b = 0 ) cuyas raíces determinan la forma de la solución, (2) La matriz compañera ( C = \begin{pmatrix} 0 & -b \\ 1 & -a end{pmatrix} ) cuyos autovalores son las raíces de la ecuación característica, y (3) El polinomio mínimo (que coincide con el característico si las raíces son simples)."
            },
            {
              "id": "AL_MEDIUM_014",
              "subCategory": "Convergencia",
              "text": "¿Cuándo converge la secuencia ( A^n ) cuando ( n \to infty )?",
              "hint": "La convergencia depende del radio espectral de ( A ).",
              "options": [
                "Cuando el radio espectral ( \rho(A) < 1 )",
                "Cuando ( det(A) \neq 0 )",
                "Cuando ( A ) es simétrica",
                "Cuando ( \text{tr}(A) > 0 )"
              ],
              "correctAnswer": "Cuando el radio espectral ( \rho(A) < 1 )",
              "explanation": "( A^n \to mathbf{0} ) cuando ( n \to infty ) si y solo si ( \rho(A) < 1 ), donde ( \rho(A) = max |lambda_i| ) es el radio espectral. Esto incluye matrices no diagonalizables. Si ( \rho(A) = 1 ), la secuencia puede diverger o permanecer acotada (e.g., si ( A ) es ortogonal)."
            },
            {
              "id": "AL_MEDIUM_015",
              "subCategory": "Forma Canónica de Jordan",
              "text": "¿Cuándo una matriz no diagonalizable necesita la forma canónica de Jordan?",
              "hint": "Usa Jordan cuando faltan autovectores para diagonalizar.",
              "options": [
                "Cuando la multiplicidad geométrica es menor que la algebraica para algún autovalor",
                "Cuando todos los autovalores son distintos",
                "Cuando la matriz es simétrica",
                "Cuando la matriz es invertible"
              ],
              "correctAnswer": "Cuando la multiplicidad geométrica es menor que la algebraica para algún autovalor",
              "explanation": "La forma de Jordan se usa cuando ( A ) no tiene suficientes autovectores (i.e., ( \text{mg}(lambda) < \text{ma}(lambda) ) para algún ( lambda )). En este caso, ( A = PJP^{-1} ), donde ( J ) es una matriz diagonal por bloques de Jordan. Cada bloque ( J_k(lambda) ) corresponde a cadenas de vectores propios generalizados."
            }
        ],
        "avanzado": [
            {
              "id": "AL_HARD_001",
              "subCategory": "Forma Canónica de Jordan",
              "text": "En la forma canónica de Jordan J = P⁻¹AP, ¿qué estructura tienen los bloques de Jordan?",
              "hint": "Considera cómo se organizan los autovectores generalizados en cadenas.",
              "options": [
                "Matrices con el autovalor en la diagonal y unos en la superdiagonal",
                "Matrices diagonales",
                "Matrices triangulares superiores con elementos arbitrarios",
                "Matrices simétricas"
              ],
              "correctAnswer": "Matrices con el autovalor en la diagonal y unos en la superdiagonal",
              "explanation": "Un bloque de Jordan Jₖ(λ) es una matriz k×k con λ en la diagonal, 1 en la superdiagonal, y 0 en otras entradas. Por ejemplo, para k=3: J₃(λ) = [[λ, 1, 0], [0, λ, 1], [0, 0, λ]]. Esta estructura surge de cadenas de autovectores generalizados que satisfacen (A - λI)ᵏv = 0. La forma de Jordan es única salvo permutación de bloques."
            },
            {
              "id": "AL_HARD_002",
              "subCategory": "Funciones de Matrices",
              "text": "Si A = PDP⁻¹, ¿cómo se calcula eᴬ?",
              "hint": "Usa la propiedad de que la exponencial matricial preserva la diagonalización.",
              "options": [
                "PeᴰP⁻¹ donde eᴰ tiene e^(λᵢ) en la diagonal",
                "eᴾeᴰeᴾ⁻¹",
                "e^(PDP⁻¹) = eᴬ directamente",
                "No se puede calcular para matrices no diagonales"
              ],
              "correctAnswer": "PeᴰP⁻¹ donde eᴰ tiene e^(λᵢ) en la diagonal",
              "explanation": "Para A diagonalizable (A = PDP⁻¹), eᴬ = PeᴰP⁻¹, donde eᴰ = diag(e^λ₁, ..., e^λₙ). Esto se deriva de la serie de Taylor de eᴬ y la propiedad P(Aⁿ)P⁻¹ = (PAP⁻¹)ⁿ. Ejemplo: Si A = [[1, 1], [0, 2]] = P[[1, 0], [0, 2]]P⁻¹, entonces eᴬ = P[[e, 0], [0, e²]]P⁻¹."
            },
            {
              "id": "AL_HARD_003",
              "subCategory": "Polinomio Mínimo",
              "text": "¿Qué relación existe entre el polinomio mínimo y el polinomio característico?",
              "hint": "Ambos anulan la matriz, pero uno tiene grado mínimo.",
              "options": [
                "El polinomio mínimo divide al polinomio característico y comparte las mismas raíces",
                "Son idénticos para todas las matrices",
                "El polinomio característico divide al mínimo",
                "No hay relación general"
              ],
              "correctAnswer": "El polinomio mínimo divide al polinomio característico y comparte las mismas raíces",
              "explanation": "El polinomio mínimo m_A(λ) es el polinomio mónico de menor grado tal que m_A(A) = 0. Divide al polinomio característico p_A(λ) y comparte sus raíces (pero con posibles multiplicidades menores). Para matrices diagonalizables, m_A(λ) = ∏ (λ - λᵢ) sin repeticiones. Ejemplo: A = I₂ tiene p_A(λ) = (λ - 1)² pero m_A(λ) = (λ - 1)."
            },
            {
              "id": "AL_HARD_004",
              "subCategory": "Teorema de Cayley-Hamilton",
              "text": "¿Qué establece el teorema de Cayley-Hamilton?",
              "hint": "Implica que una matriz satisface una ecuación polinomial específica.",
              "options": [
                "Toda matriz cuadrada satisface su polinomio característico: p_A(A) = 0",
                "Solo matrices simétricas satisfacen p_A(A) = 0",
                "p_A(A) es siempre invertible",
                "El polinomio mínimo nunca coincide con el característico"
              ],
              "correctAnswer": "Toda matriz cuadrada satisface su polinomio característico: p_A(A) = 0",
              "explanation": "El teorema de Cayley-Hamilton establece que para cualquier matriz A ∈ ℂⁿˣⁿ, si p_A(λ) = det(λI - A), entonces p_A(A) = 0. Esto permite expresar Aⁿ como combinación lineal de I, A, ..., Aⁿ⁻¹. Ejemplo: Para A = [[2, 1], [0, 1]], p_A(λ) = (λ - 2)(λ - 1), y se verifica que (A - 2I)(A - I) = 0."
            },
            {
              "id": "AL_HARD_005",
              "subCategory": "Ecuaciones en Diferencias No Homogéneas",
              "text": "Para resolver xₙ₊₁ = Axₙ + bₙ, ¿qué método es más general?",
              "hint": "Combina soluciones homogéneas y particulares adaptadas a la no homogeneidad.",
              "options": [
                "Variación de parámetros o función de Green discreta",
                "Diagonalización directa de A",
                "Eliminación gaussiana en cada paso",
                "Método de Newton-Raphson"
              ],
              "correctAnswer": "Variación de parámetros o función de Green discreta",
              "explanation": "El método de variación de parámetros usa la solución homogénea xₙ⁽ʰ⁾ = Aⁿx₀ y encuentra una solución particular xₙ⁽ᵖ⁾ usando la función de Green (o kernel discreto): xₙ⁽ᵖ⁾ = ∑ₖ₌₀ⁿ⁻¹ Aⁿ⁻¹⁻ᵏbₖ. Esto generaliza el caso escalar y funciona incluso si bₙ depende de n. La diagonalización solo es útil si A es diagonalizable y bₙ tiene forma simple."
            },
            {
              "id": "AL_HARD_006",
              "subCategory": "Estabilidad Asintótica",
              "text": "¿Qué condición garantiza la estabilidad asintótica de xₙ₊₁ = Axₙ + Buₙ?",
              "hint": "La estabilidad depende solo de la matriz de transición A.",
              "options": [
                "ρ(A) < 1 (radio espectral)",
                "Todos los autovalores de A tienen parte real negativa",
                "A es simétrica y definida positiva",
                "det(A) < 1"
              ],
              "correctAnswer": "ρ(A) < 1 (radio espectral)",
              "explanation": "El sistema es estable asintóticamente si limₙ→∞ xₙ = 0 para cualquier x₀ y uₙ = 0. Esto ocurre si y solo si ρ(A) < 1 (todos los autovalores |λᵢ| < 1). Para sistemas con control uₙ, se requiere adicionalmente controlabilidad. Nota: En tiempo continuo, se requiere Re(λᵢ) < 0."
            },
            {
              "id": "AL_HARD_007",
              "subCategory": "Controlabilidad",
              "text": "Un sistema xₙ₊₁ = Axₙ + Buₙ es controlable si:",
              "hint": "Implica que cualquier estado puede alcanzarse con una secuencia de controles adecuada.",
              "options": [
                "La matriz de controlabilidad C = [B AB ... Aⁿ⁻¹B] tiene rango n",
                "A es diagonalizable",
                "B es invertible",
                "A y B conmutan"
              ],
              "correctAnswer": "La matriz de controlabilidad C = [B AB ... Aⁿ⁻¹B] tiene rango n",
              "explanation": "El sistema es controlable si la matriz C ∈ ℝⁿˣⁿᵐ tiene rango n (Kalman, 1960). Esto permite encontrar controles u₀, ..., uₙ₋₁ que lleven x₀ a cualquier x*. Ejemplo: Para A = [[1, 1], [0, 1]] y B = [[0], [1]], C = [[0, 1], [1, 1]] tiene rango 2 → controlable."
            },
            {
              "id": "AL_HARD_008",
              "subCategory": "Observabilidad",
              "text": "Un sistema xₙ₊₁ = Axₙ, yₙ = Cxₙ es observable si:",
              "hint": "Requiere que el estado inicial pueda determinarse a partir de las salidas.",
              "options": [
                "La matriz de observabilidad O = [Cᵀ (CA)ᵀ ... (CAⁿ⁻¹)ᵀ]ᵀ tiene rango n",
                "C es invertible",
                "A es diagonal",
                "El sistema es controlable"
              ],
              "correctAnswer": "La matriz de observabilidad O = [Cᵀ (CA)ᵀ ... (CAⁿ⁻¹)ᵀ]ᵀ tiene rango n",
              "explanation": "El sistema es observable si O tiene rango n, permitiendo reconstruir x₀ a partir de y₀, ..., yₙ₋₁. Dualidad: (A, C) es observable sii (Aᵀ, Cᵀ) es controlable. Ejemplo: A = [[1, 1], [0, 1]], C = [1, 0] → O = [[1, 0], [1, 1]] tiene rango 2 → observable."
            },
            {
              "id": "AL_HARD_009",
              "subCategory": "Formas Cuadráticas Indefinidas",
              "text": "Una forma cuadrática Q(x) = xᵀAx es indefinida si:",
              "hint": "Implica que Q(x) puede tomar valores positivos y negativos.",
              "options": [
                "A tiene autovalores positivos y negativos",
                "Todos los autovalores son cero",
                "A es nilpotente",
                "A es ortogonal"
              ],
              "correctAnswer": "A tiene autovalores positivos y negativos",
              "explanation": "Q(x) es indefinida si existen x₁, x₂ tales que Q(x₁) > 0 y Q(x₂) < 0. Esto ocurre sii A tiene autovalores λᵢ > 0 y λⱼ < 0. Ejemplo: A = diag(1, -1) con Q(x) = x₁² - x₂² (punto silla). El criterio de Sylvester falla en este caso."
            },
            {
              "id": "AL_HARD_010",
              "subCategory": "Optimización",
              "text": "¿Cuál es la condición de segundo orden para un mínimo local en f(x) = xᵀAx + bᵀx + c?",
              "hint": "La matriz Hessiana debe ser definida positiva en el punto crítico.",
              "options": [
                "∇f(x*) = 0 y A definida positiva",
                "∇f(x*) = 0 y A semidefinida positiva",
                "f(x*) = 0 y tr(A) > 0",
                "A simétrica"
              ],
              "correctAnswer": "∇f(x*) = 0 y A definida positiva",
              "explanation": "Para f(x) cuadrática: (1) Condición necesaria de primer orden: ∇f(x*) = 2Ax* + b = 0. (2) Condición suficiente de segundo orden: Hess(f) = 2A definida positiva (autovalores positivos). Ejemplo: f(x₁, x₂) = x₁² + x₂² tiene mínimo en (0,0) con A = I definida positiva."
            },
            {
              "id": "AL_HARD_011",
              "subCategory": "Valores Singulares",
              "text": "¿Cómo se relacionan los valores singulares σᵢ de A con los autovalores de AᵀA?",
              "hint": "Los valores singulares son normas de transformación en direcciones principales.",
              "options": [
                "σᵢ = √(λᵢ(AᵀA)) donde λᵢ ≥ 0",
                "σᵢ = λᵢ(AᵀA)",
                "σᵢ = λᵢ(A)²",
                "No hay relación general"
              ],
              "correctAnswer": "σᵢ = √(λᵢ(AᵀA)) donde λᵢ ≥ 0",
              "explanation": "Los valores singulares σᵢ de A ∈ ℝᵐˣⁿ son las raíces cuadradas de los autovalores λᵢ de AᵀA (matriz semidefinida positiva). Si A ∈ ℂᵐˣⁿ, usar A*A. La SVD de A es A = UΣVᵀ, donde Σ = diag(σ₁, ..., σᵣ) y r = rango(A). Los σᵢ miden la amplificación de A en direcciones dadas por las columnas de V."
            },
            {
              "id": "AL_HARD_012",
              "subCategory": "Pseudoinversa",
              "text": "¿Cómo se calcula la pseudoinversa de Moore-Penrose A⁺ usando SVD?",
              "hint": "Invierte los valores singulares no nulos y transpone las matrices unitarias.",
              "options": [
                "A⁺ = VΣ⁺Uᵀ donde Σ⁺ = diag(1/σ₁, ..., 1/σᵣ, 0, ..., 0)",
                "A⁺ = UΣVᵀ",
                "A⁺ = (AᵀA)⁻¹Aᵀ (solo si AᵀA es invertible)",
                "A⁺ = Aᵀ"
              ],
              "correctAnswer": "A⁺ = VΣ⁺Uᵀ donde Σ⁺ = diag(1/σ₁, ..., 1/σᵣ, 0, ..., 0)",
              "explanation": "Si A = UΣVᵀ es la SVD, entonces A⁺ = VΣ⁺Uᵀ, donde Σ⁺ reemplaza σᵢ ≠ 0 por 1/σᵢ y transpone la matriz. Propiedades: (1) AA⁺A = A, (2) A⁺AA⁺ = A⁺, (3) (AA⁺)ᵀ = AA⁺, (4) (A⁺A)ᵀ = A⁺A. Ejemplo: Si A = [[1, 0], [0, 0]], entonces A⁺ = [[1, 0], [0, 0]]."
            },
            {
              "id": "AL_HARD_013",
              "subCategory": "Matrices de Markov",
              "text": "¿Qué propiedad tienen los autovalores de una matriz de transición de Markov P?",
              "hint": "Reflejan la conservación de probabilidad y convergencia a estado estacionario.",
              "options": [
                "|λᵢ| ≤ 1 con λ₁ = 1 y autovector asociado con entradas no negativas",
                "Todos λᵢ = 1",
                "Todos λᵢ < 1 en módulo",
                "Todos λᵢ son complejos"
              ],
              "correctAnswer": "|λᵢ| ≤ 1 con λ₁ = 1 y autovector asociado con entradas no negativas",
              "explanation": "Para P estocástica (Pᵢⱼ ≥ 0, ∑ⱼ Pᵢⱼ = 1): (1) ρ(P) = 1, (2) Existe π ≥ 0 con πᵀP = πᵀ (vector de estado estacionario), (3) Si P es irreducible y aperiódica, limₙ→∞ Pⁿ = 1πᵀ. Ejemplo: P = [[0.9, 0.1], [0.5, 0.5]] tiene λ₁ = 1 con π = (5/6, 1/6)."
            },
            {
              "id": "AL_HARD_014",
              "subCategory": "Teorema de Perron-Frobenius",
              "text": "Para una matriz positiva A > 0, ¿qué establece el teorema de Perron-Frobenius?",
              "hint": "Garantiza un autovalor dominante con propiedades especiales.",
              "options": [
                "Existe un autovalor simple λ₁ = ρ(A) > 0 con autovector positivo",
                "Todos los autovalores son iguales",
                "No hay autovalores reales",
                "A es diagonalizable"
              ],
              "correctAnswer": "Existe un autovalor simple λ₁ = ρ(A) > 0 con autovector positivo",
              "explanation": "El teorema de Perron-Frobenius para A > 0 establece: (1) ρ(A) es autovalor simple (multiplicidad algebraica 1), (2) Existe autovector v > 0 con Av = ρ(A)v, (3) |λᵢ| < ρ(A) para otros autovalores. Aplicaciones: Cadenas de Markov, modelos de crecimiento poblacional."
            },
            {
              "id": "AL_HARD_015",
              "subCategory": "Análisis Espectral",
              "text": "¿Qué información proporciona el radio espectral ρ(A) sobre Aⁿ?",
              "hint": "Determina el comportamiento asintótico de las potencias matriciales.",
              "options": [
                "Si ρ(A) < 1, Aⁿ → 0 exponencialmente; si ρ(A) > 1, ||Aⁿ|| → ∞",
                "Solo determina la traza de Aⁿ",
                "Es irrelevante para matrices no normales",
                "ρ(A) = tr(A)/n"
              ],
              "correctAnswer": "Si ρ(A) < 1, Aⁿ → 0 exponencialmente; si ρ(A) > 1, ||Aⁿ|| → ∞",
              "explanation": "El radio espectral ρ(A) = max |λᵢ| determina la tasa de crecimiento/decaimiento de Aⁿ: (1) Si ρ(A) < 1, ∃C > 0 tal que ||Aⁿ|| ≤ Cρ(A)ⁿ → 0. (2) Si ρ(A) > 1, ||Aⁿ|| → ∞. (3) Si ρ(A) = 1, el comportamiento depende de la multiplicidad de los autovalores con |λᵢ| = 1."
            }
          ]
      }
    }
  ]
}